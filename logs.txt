* 
* ==> Audit <==
* |-----------|----------------------|----------|-------------|---------|----------------------|----------------------|
|  Command  |         Args         | Profile  |    User     | Version |      Start Time      |       End Time       |
|-----------|----------------------|----------|-------------|---------|----------------------|----------------------|
| start     |                      | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 16:14 CEST | 29 Jul 22 16:16 CEST |
| stop      |                      | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 16:19 CEST | 29 Jul 22 16:19 CEST |
| start     | --nodes=2            | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 16:20 CEST | 29 Jul 22 16:21 CEST |
| stop      |                      | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 16:23 CEST | 29 Jul 22 16:23 CEST |
| start     | --nodes=2            | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 16:23 CEST | 29 Jul 22 16:24 CEST |
| dashboard |                      | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 16:49 CEST |                      |
| stop      |                      | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 17:09 CEST | 29 Jul 22 17:09 CEST |
| start     | --nodes=2            | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 17:10 CEST | 29 Jul 22 17:11 CEST |
| get       | nodes                | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 17:11 CEST |                      |
| dashboard |                      | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 17:12 CEST |                      |
| stop      |                      | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 18:46 CEST | 29 Jul 22 18:46 CEST |
| dashboard |                      | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 20:07 CEST |                      |
| dashboard |                      | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 20:08 CEST |                      |
| start     | --vm-driver=hyperkit | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 20:09 CEST | 29 Jul 22 20:10 CEST |
| dashboard |                      | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 20:15 CEST |                      |
| stop      |                      | minikube | xiaolinsong | v1.26.0 | 29 Jul 22 20:18 CEST | 29 Jul 22 20:18 CEST |
| start     | --vm-driver=hyperkit | minikube | xiaolinsong | v1.26.0 | 30 Jul 22 15:37 CEST | 30 Jul 22 15:38 CEST |
| dashboard |                      | minikube | xiaolinsong | v1.26.0 | 30 Jul 22 15:39 CEST |                      |
| addons    | list                 | minikube | xiaolinsong | v1.26.0 | 30 Jul 22 15:48 CEST | 30 Jul 22 15:48 CEST |
| stop      |                      | minikube | xiaolinsong | v1.26.0 | 30 Jul 22 20:39 CEST | 30 Jul 22 20:40 CEST |
| start     | --vm-driver=hyperkit | minikube | xiaolinsong | v1.26.0 | 31 Jul 22 13:58 CEST | 31 Jul 22 13:59 CEST |
| start     | --vm-driver=hyperkit | minikube | xiaolinsong | v1.26.0 | 03 Aug 22 11:10 CEST | 03 Aug 22 11:11 CEST |
|-----------|----------------------|----------|-------------|---------|----------------------|----------------------|

* 
* ==> Last Start <==
* Log file created at: 2022/08/03 11:10:44
Running on machine: xiaolins-mbp
Binary: Built with gc go1.18.3 for darwin/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0803 11:10:44.908373    1342 out.go:296] Setting OutFile to fd 1 ...
I0803 11:10:44.908930    1342 out.go:348] isatty.IsTerminal(1) = true
I0803 11:10:44.908935    1342 out.go:309] Setting ErrFile to fd 2...
I0803 11:10:44.908945    1342 out.go:348] isatty.IsTerminal(2) = true
I0803 11:10:44.910468    1342 root.go:329] Updating PATH: /Users/xiaolinsong/.minikube/bin
W0803 11:10:44.910992    1342 root.go:307] Error reading config file at /Users/xiaolinsong/.minikube/config/config.json: open /Users/xiaolinsong/.minikube/config/config.json: no such file or directory
I0803 11:10:44.911290    1342 out.go:303] Setting JSON to false
I0803 11:10:44.947159    1342 start.go:115] hostinfo: {"hostname":"xiaolins-mbp.home","uptime":376,"bootTime":1659517468,"procs":425,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"12.5","kernelVersion":"21.6.0","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"af873903-d1dc-5468-98df-d93d3f58102e"}
W0803 11:10:44.947419    1342 start.go:123] gopshost.Virtualization returned error: not implemented yet
I0803 11:10:44.966619    1342 out.go:177] üòÑ  minikube v1.26.0 on Darwin 12.5
I0803 11:10:45.020549    1342 notify.go:193] Checking for updates...
I0803 11:10:45.028080    1342 config.go:178] Loaded profile config "minikube": Driver=hyperkit, ContainerRuntime=docker, KubernetesVersion=v1.24.1
I0803 11:10:45.029585    1342 main.go:134] libmachine: Found binary path at /Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit
I0803 11:10:45.029928    1342 main.go:134] libmachine: Launching plugin server for driver hyperkit
I0803 11:10:45.114430    1342 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:49388
I0803 11:10:45.118132    1342 main.go:134] libmachine: () Calling .GetVersion
I0803 11:10:45.119118    1342 main.go:134] libmachine: Using API Version  1
I0803 11:10:45.119140    1342 main.go:134] libmachine: () Calling .SetConfigRaw
I0803 11:10:45.119611    1342 main.go:134] libmachine: () Calling .GetMachineName
I0803 11:10:45.119759    1342 main.go:134] libmachine: (minikube) Calling .DriverName
I0803 11:10:45.121508    1342 driver.go:360] Setting default libvirt URI to qemu:///system
I0803 11:10:45.121883    1342 main.go:134] libmachine: Found binary path at /Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit
I0803 11:10:45.121918    1342 main.go:134] libmachine: Launching plugin server for driver hyperkit
I0803 11:10:45.131542    1342 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:49390
I0803 11:10:45.132052    1342 main.go:134] libmachine: () Calling .GetVersion
I0803 11:10:45.132563    1342 main.go:134] libmachine: Using API Version  1
I0803 11:10:45.132573    1342 main.go:134] libmachine: () Calling .SetConfigRaw
I0803 11:10:45.132880    1342 main.go:134] libmachine: () Calling .GetMachineName
I0803 11:10:45.133046    1342 main.go:134] libmachine: (minikube) Calling .DriverName
I0803 11:10:45.167431    1342 lock.go:35] WriteFile acquiring /Users/xiaolinsong/.minikube/last_update_check: {Name:mk03835a7b21c869709f47d1920f9a14598581ca Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0803 11:10:45.178105    1342 out.go:177] üéâ  minikube 1.26.1 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.26.1
I0803 11:10:45.369656    1342 out.go:177] ‚ú®  Using the hyperkit driver based on existing profile
I0803 11:10:45.383108    1342 start.go:284] selected driver: hyperkit
I0803 11:10:45.496275    1342 out.go:177] üí°  To disable this notice, run: 'minikube config set WantUpdateNotification false'

I0803 11:10:45.497553    1342 start.go:805] validating driver "hyperkit" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.26.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.32@sha256:9190bd2393eae887316c97a74370b7d5dad8f0b2ef91ac2662bc36f7ef8e0b95 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.64.2 Port:8443 KubernetesVersion:v1.24.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0803 11:10:45.613590    1342 start.go:816] status for hyperkit: {Installed:true Healthy:true Running:true NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0803 11:10:45.613705    1342 install.go:52] acquiring lock: {Name:mk4023283b30b374c3f04c8805d539e68824c0b8 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0803 11:10:45.614010    1342 install.go:117] Validating docker-machine-driver-hyperkit, PATH=/Users/xiaolinsong/.minikube/bin:/Users/xiaolinsong/.nvm/versions/node/v16.15.1/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/xiaolinsong/dev/tools/apache-maven-3.3.9/bin:/usr/local/mysql/bin
I0803 11:10:45.623511    1342 install.go:137] /Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit version is 1.26.0
I0803 11:10:45.634535    1342 install.go:79] stdout: /Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit
I0803 11:10:45.634557    1342 install.go:81] /Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit looks good
I0803 11:10:45.634794    1342 cni.go:95] Creating CNI manager for ""
I0803 11:10:45.634802    1342 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0803 11:10:45.635125    1342 start_flags.go:310] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.26.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.32@sha256:9190bd2393eae887316c97a74370b7d5dad8f0b2ef91ac2662bc36f7ef8e0b95 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.64.2 Port:8443 KubernetesVersion:v1.24.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0803 11:10:45.635712    1342 iso.go:128] acquiring lock: {Name:mked35537b2f8417794a346b8c88fad4562fe857 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0803 11:10:45.721080    1342 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I0803 11:10:45.732105    1342 preload.go:132] Checking if preload exists for k8s version v1.24.1 and runtime docker
I0803 11:10:45.732210    1342 preload.go:148] Found local preload: /Users/xiaolinsong/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.1-docker-overlay2-amd64.tar.lz4
I0803 11:10:45.735368    1342 cache.go:57] Caching tarball of preloaded images
I0803 11:10:45.735696    1342 preload.go:174] Found /Users/xiaolinsong/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.24.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0803 11:10:45.735711    1342 cache.go:60] Finished verifying existence of preloaded tar for  v1.24.1 on docker
I0803 11:10:45.735871    1342 profile.go:148] Saving config to /Users/xiaolinsong/.minikube/profiles/minikube/config.json ...
I0803 11:10:45.736449    1342 cache.go:208] Successfully downloaded all kic artifacts
I0803 11:10:45.736483    1342 start.go:352] acquiring machines lock for minikube: {Name:mke7d0be842954122d1fb77f583be9abcdb3c7ea Clock:{} Delay:500ms Timeout:13m0s Cancel:<nil>}
I0803 11:10:45.736634    1342 start.go:356] acquired machines lock for "minikube" in 139.051¬µs
I0803 11:10:45.736666    1342 start.go:94] Skipping create...Using existing machine configuration
I0803 11:10:45.736680    1342 fix.go:55] fixHost starting: 
I0803 11:10:45.737090    1342 main.go:134] libmachine: Found binary path at /Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit
I0803 11:10:45.737129    1342 main.go:134] libmachine: Launching plugin server for driver hyperkit
I0803 11:10:45.746101    1342 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:49392
I0803 11:10:45.746693    1342 main.go:134] libmachine: () Calling .GetVersion
I0803 11:10:45.747190    1342 main.go:134] libmachine: Using API Version  1
I0803 11:10:45.747202    1342 main.go:134] libmachine: () Calling .SetConfigRaw
I0803 11:10:45.747518    1342 main.go:134] libmachine: () Calling .GetMachineName
I0803 11:10:45.747728    1342 main.go:134] libmachine: (minikube) Calling .DriverName
I0803 11:10:45.747891    1342 main.go:134] libmachine: (minikube) Calling .GetState
I0803 11:10:45.748033    1342 main.go:134] libmachine: (minikube) DBG | exe=/Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0803 11:10:45.748807    1342 main.go:134] libmachine: (minikube) DBG | hyperkit pid from json: 2522
I0803 11:10:45.750180    1342 main.go:134] libmachine: (minikube) DBG | hyperkit pid 2522 missing from process table
I0803 11:10:45.750284    1342 fix.go:103] recreateIfNeeded on minikube: state=Stopped err=<nil>
I0803 11:10:45.750307    1342 main.go:134] libmachine: (minikube) Calling .DriverName
W0803 11:10:45.750474    1342 fix.go:129] unexpected machine state, will restart: <nil>
I0803 11:10:45.780061    1342 out.go:177] üîÑ  Restarting existing hyperkit VM for "minikube" ...
I0803 11:10:45.792080    1342 main.go:134] libmachine: (minikube) Calling .Start
I0803 11:10:45.792329    1342 main.go:134] libmachine: (minikube) DBG | exe=/Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0803 11:10:45.792496    1342 main.go:134] libmachine: (minikube) minikube might have been shutdown in an unclean way, the hyperkit pid file still exists: /Users/xiaolinsong/.minikube/machines/minikube/hyperkit.pid
I0803 11:10:45.794958    1342 main.go:134] libmachine: (minikube) DBG | hyperkit pid 2522 missing from process table
I0803 11:10:45.794968    1342 main.go:134] libmachine: (minikube) DBG | pid 2522 is in state "Stopped"
I0803 11:10:45.794984    1342 main.go:134] libmachine: (minikube) DBG | Removing stale pid file /Users/xiaolinsong/.minikube/machines/minikube/hyperkit.pid...
I0803 11:10:45.795254    1342 main.go:134] libmachine: (minikube) DBG | Using UUID eae7a0fa-0f48-11ed-8d0e-f40f241f4ec8
I0803 11:10:49.439147    1342 main.go:134] libmachine: (minikube) DBG | Generated MAC 8a:7c:72:5d:74:61
I0803 11:10:49.439178    1342 main.go:134] libmachine: (minikube) DBG | Starting with cmdline: loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube
I0803 11:10:49.440250    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:49 DEBUG: hyperkit: Start &hyperkit.HyperKit{HyperKit:"/usr/local/bin/hyperkit", Argv0:"", StateDir:"/Users/xiaolinsong/.minikube/machines/minikube", VPNKitSock:"", VPNKitUUID:"", VPNKitPreferredIPv4:"", UUID:"eae7a0fa-0f48-11ed-8d0e-f40f241f4ec8", Disks:[]hyperkit.Disk{(*hyperkit.RawDisk)(0xc0003e8930)}, ISOImages:[]string{"/Users/xiaolinsong/.minikube/machines/minikube/boot2docker.iso"}, VSock:false, VSockDir:"", VSockPorts:[]int(nil), VSockGuestCID:3, VMNet:true, Sockets9P:[]hyperkit.Socket9P(nil), Kernel:"/Users/xiaolinsong/.minikube/machines/minikube/bzimage", Initrd:"/Users/xiaolinsong/.minikube/machines/minikube/initrd", Bootrom:"", CPUs:2, Memory:4000, Console:1, Serials:[]hyperkit.Serial(nil), Pid:0, Arguments:[]string(nil), CmdLine:"", process:(*os.Process)(nil)}
I0803 11:10:49.440310    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:49 DEBUG: hyperkit: check &hyperkit.HyperKit{HyperKit:"/usr/local/bin/hyperkit", Argv0:"", StateDir:"/Users/xiaolinsong/.minikube/machines/minikube", VPNKitSock:"", VPNKitUUID:"", VPNKitPreferredIPv4:"", UUID:"eae7a0fa-0f48-11ed-8d0e-f40f241f4ec8", Disks:[]hyperkit.Disk{(*hyperkit.RawDisk)(0xc0003e8930)}, ISOImages:[]string{"/Users/xiaolinsong/.minikube/machines/minikube/boot2docker.iso"}, VSock:false, VSockDir:"", VSockPorts:[]int(nil), VSockGuestCID:3, VMNet:true, Sockets9P:[]hyperkit.Socket9P(nil), Kernel:"/Users/xiaolinsong/.minikube/machines/minikube/bzimage", Initrd:"/Users/xiaolinsong/.minikube/machines/minikube/initrd", Bootrom:"", CPUs:2, Memory:4000, Console:1, Serials:[]hyperkit.Serial(nil), Pid:0, Arguments:[]string(nil), CmdLine:"", process:(*os.Process)(nil)}
I0803 11:10:49.440684    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:49 DEBUG: hyperkit: Arguments: []string{"-A", "-u", "-F", "/Users/xiaolinsong/.minikube/machines/minikube/hyperkit.pid", "-c", "2", "-m", "4000M", "-s", "0:0,hostbridge", "-s", "31,lpc", "-s", "1:0,virtio-net", "-U", "eae7a0fa-0f48-11ed-8d0e-f40f241f4ec8", "-s", "2:0,virtio-blk,/Users/xiaolinsong/.minikube/machines/minikube/minikube.rawdisk", "-s", "3,ahci-cd,/Users/xiaolinsong/.minikube/machines/minikube/boot2docker.iso", "-s", "4,virtio-rnd", "-l", "com1,autopty=/Users/xiaolinsong/.minikube/machines/minikube/tty,log=/Users/xiaolinsong/.minikube/machines/minikube/console-ring", "-f", "kexec,/Users/xiaolinsong/.minikube/machines/minikube/bzimage,/Users/xiaolinsong/.minikube/machines/minikube/initrd,earlyprintk=serial loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube"}
I0803 11:10:49.440769    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:49 DEBUG: hyperkit: CmdLine: "/usr/local/bin/hyperkit -A -u -F /Users/xiaolinsong/.minikube/machines/minikube/hyperkit.pid -c 2 -m 4000M -s 0:0,hostbridge -s 31,lpc -s 1:0,virtio-net -U eae7a0fa-0f48-11ed-8d0e-f40f241f4ec8 -s 2:0,virtio-blk,/Users/xiaolinsong/.minikube/machines/minikube/minikube.rawdisk -s 3,ahci-cd,/Users/xiaolinsong/.minikube/machines/minikube/boot2docker.iso -s 4,virtio-rnd -l com1,autopty=/Users/xiaolinsong/.minikube/machines/minikube/tty,log=/Users/xiaolinsong/.minikube/machines/minikube/console-ring -f kexec,/Users/xiaolinsong/.minikube/machines/minikube/bzimage,/Users/xiaolinsong/.minikube/machines/minikube/initrd,earlyprintk=serial loglevel=3 console=ttyS0 console=tty0 noembed nomodeset norestore waitusb=10 systemd.legacy_systemd_cgroup_controller=yes random.trust_cpu=on hw_rng_model=virtio base host=minikube"
I0803 11:10:49.440814    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:49 DEBUG: hyperkit: Redirecting stdout/stderr to logger
I0803 11:10:49.442775    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:49 DEBUG: hyperkit: Pid is 1359
I0803 11:10:49.443750    1342 main.go:134] libmachine: (minikube) DBG | Attempt 0
I0803 11:10:49.443773    1342 main.go:134] libmachine: (minikube) DBG | exe=/Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0803 11:10:49.444133    1342 main.go:134] libmachine: (minikube) DBG | hyperkit pid from json: 1359
I0803 11:10:49.447114    1342 main.go:134] libmachine: (minikube) DBG | Searching for 8a:7c:72:5d:74:61 in /var/db/dhcpd_leases ...
I0803 11:10:49.447469    1342 main.go:134] libmachine: (minikube) DBG | Found 1 entries in /var/db/dhcpd_leases!
I0803 11:10:49.447514    1342 main.go:134] libmachine: (minikube) DBG | dhcp entry: {Name:minikube IPAddress:192.168.64.2 HWAddress:8a:7c:72:5d:74:61 ID:1,8a:7c:72:5d:74:61 Lease:0x62e7bfe1}
I0803 11:10:49.447568    1342 main.go:134] libmachine: (minikube) DBG | Found match: 8a:7c:72:5d:74:61
I0803 11:10:49.447595    1342 main.go:134] libmachine: (minikube) DBG | IP: 192.168.64.2
I0803 11:10:49.447667    1342 main.go:134] libmachine: (minikube) Calling .GetConfigRaw
I0803 11:10:49.449069    1342 main.go:134] libmachine: (minikube) Calling .GetIP
I0803 11:10:49.449392    1342 profile.go:148] Saving config to /Users/xiaolinsong/.minikube/profiles/minikube/config.json ...
I0803 11:10:49.450022    1342 machine.go:88] provisioning docker machine ...
I0803 11:10:49.450039    1342 main.go:134] libmachine: (minikube) Calling .DriverName
I0803 11:10:49.450245    1342 main.go:134] libmachine: (minikube) Calling .GetMachineName
I0803 11:10:49.450400    1342 buildroot.go:166] provisioning hostname "minikube"
I0803 11:10:49.450417    1342 main.go:134] libmachine: (minikube) Calling .GetMachineName
I0803 11:10:49.450555    1342 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I0803 11:10:49.450682    1342 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I0803 11:10:49.450801    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:10:49.450973    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:10:49.451128    1342 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I0803 11:10:49.452658    1342 main.go:134] libmachine: Using SSH client type: native
I0803 11:10:49.453355    1342 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43da180] 0x43dd1e0 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I0803 11:10:49.453365    1342 main.go:134] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0803 11:10:49.481616    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:49 INFO : hyperkit: stderr: Using fd 5 for I/O notifications
I0803 11:10:50.109131    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:50 INFO : hyperkit: stderr: /Users/xiaolinsong/.minikube/machines/minikube/boot2docker.iso: fcntl(F_PUNCHHOLE) Operation not permitted: block device will not support TRIM/DISCARD
I0803 11:10:50.110414    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:50 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 12 unspecified don't care: bit is 0
I0803 11:10:50.110431    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:50 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 20 unspecified don't care: bit is 0
I0803 11:10:50.110442    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:50 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 3 bit: 13 unspecified don't care: bit is 0
I0803 11:10:51.156059    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:51 INFO : hyperkit: stderr: rdmsr to register 0x3a on vcpu 0
I0803 11:10:51.156091    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:51 INFO : hyperkit: stderr: rdmsr to register 0x140 on vcpu 0
I0803 11:10:51.262312    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:51 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 12 unspecified don't care: bit is 0
I0803 11:10:51.262336    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:51 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 4 bit: 20 unspecified don't care: bit is 0
I0803 11:10:51.262357    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:51 INFO : hyperkit: stderr: vmx_set_ctlreg: cap_field: 3 bit: 13 unspecified don't care: bit is 0
I0803 11:10:51.263333    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:51 INFO : hyperkit: stderr: rdmsr to register 0x3a on vcpu 1
I0803 11:10:51.263344    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:51 INFO : hyperkit: stderr: rdmsr to register 0x140 on vcpu 1
I0803 11:10:57.737614    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:57 INFO : hyperkit: stderr: rdmsr to register 0x64d on vcpu 1
I0803 11:10:57.737710    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:57 INFO : hyperkit: stderr: rdmsr to register 0x64e on vcpu 1
I0803 11:10:57.737723    1342 main.go:134] libmachine: (minikube) DBG | 2022/08/03 11:10:57 INFO : hyperkit: stderr: rdmsr to register 0x34 on vcpu 1
I0803 11:11:08.575337    1342 main.go:134] libmachine: SSH cmd err, output: <nil>: minikube

I0803 11:11:08.575361    1342 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I0803 11:11:08.575575    1342 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I0803 11:11:08.575719    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:08.575843    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:08.575965    1342 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I0803 11:11:08.576137    1342 main.go:134] libmachine: Using SSH client type: native
I0803 11:11:08.576329    1342 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43da180] 0x43dd1e0 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I0803 11:11:08.576341    1342 main.go:134] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0803 11:11:08.671655    1342 main.go:134] libmachine: SSH cmd err, output: <nil>: 
I0803 11:11:08.671679    1342 buildroot.go:172] set auth options {CertDir:/Users/xiaolinsong/.minikube CaCertPath:/Users/xiaolinsong/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/xiaolinsong/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/xiaolinsong/.minikube/machines/server.pem ServerKeyPath:/Users/xiaolinsong/.minikube/machines/server-key.pem ClientKeyPath:/Users/xiaolinsong/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/xiaolinsong/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/xiaolinsong/.minikube}
I0803 11:11:08.671711    1342 buildroot.go:174] setting up certificates
I0803 11:11:08.671724    1342 provision.go:83] configureAuth start
I0803 11:11:08.671740    1342 main.go:134] libmachine: (minikube) Calling .GetMachineName
I0803 11:11:08.671948    1342 main.go:134] libmachine: (minikube) Calling .GetIP
I0803 11:11:08.672131    1342 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I0803 11:11:08.672305    1342 provision.go:138] copyHostCerts
I0803 11:11:08.672482    1342 exec_runner.go:144] found /Users/xiaolinsong/.minikube/ca.pem, removing ...
I0803 11:11:08.674908    1342 exec_runner.go:207] rm: /Users/xiaolinsong/.minikube/ca.pem
I0803 11:11:08.678590    1342 exec_runner.go:151] cp: /Users/xiaolinsong/.minikube/certs/ca.pem --> /Users/xiaolinsong/.minikube/ca.pem (1090 bytes)
I0803 11:11:08.680224    1342 exec_runner.go:144] found /Users/xiaolinsong/.minikube/cert.pem, removing ...
I0803 11:11:08.680234    1342 exec_runner.go:207] rm: /Users/xiaolinsong/.minikube/cert.pem
I0803 11:11:08.680360    1342 exec_runner.go:151] cp: /Users/xiaolinsong/.minikube/certs/cert.pem --> /Users/xiaolinsong/.minikube/cert.pem (1135 bytes)
I0803 11:11:08.696213    1342 exec_runner.go:144] found /Users/xiaolinsong/.minikube/key.pem, removing ...
I0803 11:11:08.696225    1342 exec_runner.go:207] rm: /Users/xiaolinsong/.minikube/key.pem
I0803 11:11:08.696358    1342 exec_runner.go:151] cp: /Users/xiaolinsong/.minikube/certs/key.pem --> /Users/xiaolinsong/.minikube/key.pem (1679 bytes)
I0803 11:11:08.696863    1342 provision.go:112] generating server cert: /Users/xiaolinsong/.minikube/machines/server.pem ca-key=/Users/xiaolinsong/.minikube/certs/ca.pem private-key=/Users/xiaolinsong/.minikube/certs/ca-key.pem org=xiaolinsong.minikube san=[192.168.64.2 192.168.64.2 localhost 127.0.0.1 minikube minikube]
I0803 11:11:08.841934    1342 provision.go:172] copyRemoteCerts
I0803 11:11:08.852171    1342 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0803 11:11:08.852201    1342 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I0803 11:11:08.852493    1342 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I0803 11:11:08.852665    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:08.852864    1342 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I0803 11:11:08.853041    1342 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/xiaolinsong/.minikube/machines/minikube/id_rsa Username:docker}
I0803 11:11:08.909797    1342 ssh_runner.go:362] scp /Users/xiaolinsong/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1090 bytes)
I0803 11:11:08.936604    1342 ssh_runner.go:362] scp /Users/xiaolinsong/.minikube/machines/server.pem --> /etc/docker/server.pem (1212 bytes)
I0803 11:11:08.964268    1342 ssh_runner.go:362] scp /Users/xiaolinsong/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0803 11:11:08.990987    1342 provision.go:86] duration metric: configureAuth took 319.246494ms
I0803 11:11:08.990999    1342 buildroot.go:189] setting minikube options for container-runtime
I0803 11:11:08.991234    1342 config.go:178] Loaded profile config "minikube": Driver=hyperkit, ContainerRuntime=docker, KubernetesVersion=v1.24.1
I0803 11:11:08.991248    1342 main.go:134] libmachine: (minikube) Calling .DriverName
I0803 11:11:08.991429    1342 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I0803 11:11:08.991553    1342 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I0803 11:11:08.991711    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:08.991835    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:08.991974    1342 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I0803 11:11:08.992143    1342 main.go:134] libmachine: Using SSH client type: native
I0803 11:11:08.992289    1342 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43da180] 0x43dd1e0 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I0803 11:11:08.992296    1342 main.go:134] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0803 11:11:09.083754    1342 main.go:134] libmachine: SSH cmd err, output: <nil>: tmpfs

I0803 11:11:09.083764    1342 buildroot.go:70] root file system type: tmpfs
I0803 11:11:09.083946    1342 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I0803 11:11:09.083984    1342 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I0803 11:11:09.084180    1342 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I0803 11:11:09.084307    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:09.084420    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:09.084591    1342 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I0803 11:11:09.084829    1342 main.go:134] libmachine: Using SSH client type: native
I0803 11:11:09.084994    1342 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43da180] 0x43dd1e0 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I0803 11:11:09.085104    1342 main.go:134] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperkit --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0803 11:11:09.191172    1342 main.go:134] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network.target  minikube-automount.service docker.socket
Requires= minikube-automount.service docker.socket 
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=hyperkit --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0803 11:11:09.191192    1342 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I0803 11:11:09.191393    1342 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I0803 11:11:09.191516    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:09.191651    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:09.191805    1342 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I0803 11:11:09.191982    1342 main.go:134] libmachine: Using SSH client type: native
I0803 11:11:09.192136    1342 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43da180] 0x43dd1e0 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I0803 11:11:09.192150    1342 main.go:134] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0803 11:11:11.588936    1342 main.go:134] libmachine: SSH cmd err, output: <nil>: diff: can't stat '/lib/systemd/system/docker.service': No such file or directory
Created symlink /etc/systemd/system/multi-user.target.wants/docker.service ‚Üí /usr/lib/systemd/system/docker.service.

I0803 11:11:11.588950    1342 machine.go:91] provisioned docker machine in 22.13892025s
I0803 11:11:11.588994    1342 start.go:306] post-start starting for "minikube" (driver="hyperkit")
I0803 11:11:11.589001    1342 start.go:316] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0803 11:11:11.589015    1342 main.go:134] libmachine: (minikube) Calling .DriverName
I0803 11:11:11.589287    1342 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0803 11:11:11.589312    1342 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I0803 11:11:11.589467    1342 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I0803 11:11:11.589590    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:11.589705    1342 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I0803 11:11:11.589829    1342 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/xiaolinsong/.minikube/machines/minikube/id_rsa Username:docker}
I0803 11:11:11.647992    1342 ssh_runner.go:195] Run: cat /etc/os-release
I0803 11:11:11.652894    1342 info.go:137] Remote host: Buildroot 2021.02.12
I0803 11:11:11.652911    1342 filesync.go:126] Scanning /Users/xiaolinsong/.minikube/addons for local assets ...
I0803 11:11:11.653046    1342 filesync.go:126] Scanning /Users/xiaolinsong/.minikube/files for local assets ...
I0803 11:11:11.653104    1342 start.go:309] post-start completed in 64.103168ms
I0803 11:11:11.653118    1342 fix.go:57] fixHost completed within 25.916439104s
I0803 11:11:11.653134    1342 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I0803 11:11:11.653336    1342 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I0803 11:11:11.653464    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:11.653600    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:11.653728    1342 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I0803 11:11:11.653887    1342 main.go:134] libmachine: Using SSH client type: native
I0803 11:11:11.654044    1342 main.go:134] libmachine: &{{{<nil> 0 [] [] []} docker [0x43da180] 0x43dd1e0 <nil>  [] 0s} 192.168.64.2 22 <nil> <nil>}
I0803 11:11:11.654051    1342 main.go:134] libmachine: About to run SSH command:
date +%!s(MISSING).%!N(MISSING)
I0803 11:11:11.749219    1342 main.go:134] libmachine: SSH cmd err, output: <nil>: 1659517871.699355462

I0803 11:11:11.749234    1342 fix.go:207] guest clock: 1659517871.699355462
I0803 11:11:11.749241    1342 fix.go:220] Guest: 2022-08-03 11:11:11.699355462 +0200 CEST Remote: 2022-08-03 11:11:11.65312 +0200 CEST m=+26.909531758 (delta=46.235462ms)
I0803 11:11:11.749263    1342 fix.go:191] guest clock delta is within tolerance: 46.235462ms
I0803 11:11:11.749267    1342 start.go:81] releasing machines lock for "minikube", held for 26.012627616s
I0803 11:11:11.749289    1342 main.go:134] libmachine: (minikube) Calling .DriverName
I0803 11:11:11.749478    1342 main.go:134] libmachine: (minikube) Calling .GetIP
I0803 11:11:11.749628    1342 main.go:134] libmachine: (minikube) Calling .DriverName
I0803 11:11:11.749769    1342 main.go:134] libmachine: (minikube) Calling .DriverName
I0803 11:11:11.749885    1342 main.go:134] libmachine: (minikube) Calling .DriverName
I0803 11:11:11.750402    1342 main.go:134] libmachine: (minikube) Calling .DriverName
I0803 11:11:11.750572    1342 main.go:134] libmachine: (minikube) Calling .DriverName
I0803 11:11:11.751032    1342 ssh_runner.go:195] Run: curl -sS -m 2 https://k8s.gcr.io/
I0803 11:11:11.751190    1342 ssh_runner.go:195] Run: systemctl --version
I0803 11:11:11.751234    1342 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I0803 11:11:11.751406    1342 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I0803 11:11:11.751443    1342 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I0803 11:11:11.751615    1342 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I0803 11:11:11.751632    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:11.751789    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:11.751829    1342 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I0803 11:11:11.751935    1342 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I0803 11:11:11.751937    1342 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/xiaolinsong/.minikube/machines/minikube/id_rsa Username:docker}
I0803 11:11:11.752042    1342 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/xiaolinsong/.minikube/machines/minikube/id_rsa Username:docker}
I0803 11:11:11.858352    1342 preload.go:132] Checking if preload exists for k8s version v1.24.1 and runtime docker
I0803 11:11:11.860577    1342 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0803 11:11:11.893503    1342 docker.go:602] Got preloaded images: -- stdout --
redis:latest
mongo:5.0
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
k8s.gcr.io/kube-apiserver:v1.24.1
k8s.gcr.io/kube-proxy:v1.24.1
k8s.gcr.io/kube-scheduler:v1.24.1
k8s.gcr.io/kube-controller-manager:v1.24.1
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/pause:3.7
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5
k8s.gcr.io/echoserver:1.10

-- /stdout --
I0803 11:11:11.893517    1342 docker.go:533] Images already preloaded, skipping extraction
I0803 11:11:11.893605    1342 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0803 11:11:11.907802    1342 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I0803 11:11:11.922059    1342 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0803 11:11:11.936691    1342 ssh_runner.go:195] Run: sudo systemctl stop -f crio
I0803 11:11:11.967144    1342 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0803 11:11:11.982436    1342 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
image-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0803 11:11:12.002725    1342 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0803 11:11:12.140627    1342 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0803 11:11:12.304996    1342 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0803 11:11:12.459249    1342 ssh_runner.go:195] Run: sudo systemctl restart docker
I0803 11:11:14.009481    1342 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.550194262s)
I0803 11:11:14.009551    1342 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0803 11:11:14.141818    1342 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0803 11:11:14.274952    1342 ssh_runner.go:195] Run: sudo systemctl start cri-docker.socket
I0803 11:11:14.292506    1342 start.go:447] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0803 11:11:14.292918    1342 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0803 11:11:14.298848    1342 start.go:468] Will wait 60s for crictl version
I0803 11:11:14.298918    1342 ssh_runner.go:195] Run: sudo crictl version
I0803 11:11:14.449516    1342 start.go:477] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  20.10.16
RuntimeApiVersion:  1.41.0
I0803 11:11:14.449574    1342 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0803 11:11:14.488655    1342 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0803 11:11:14.550524    1342 out.go:204] üê≥  Preparing Kubernetes v1.24.1 on Docker 20.10.16 ...
I0803 11:11:14.552478    1342 ssh_runner.go:195] Run: grep 192.168.64.1	host.minikube.internal$ /etc/hosts
I0803 11:11:14.557545    1342 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.64.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0803 11:11:14.573005    1342 preload.go:132] Checking if preload exists for k8s version v1.24.1 and runtime docker
I0803 11:11:14.573065    1342 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0803 11:11:14.611145    1342 docker.go:602] Got preloaded images: -- stdout --
redis:latest
mongo:5.0
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
k8s.gcr.io/kube-apiserver:v1.24.1
k8s.gcr.io/kube-controller-manager:v1.24.1
k8s.gcr.io/kube-proxy:v1.24.1
k8s.gcr.io/kube-scheduler:v1.24.1
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/pause:3.7
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5
k8s.gcr.io/echoserver:1.10

-- /stdout --
I0803 11:11:14.611155    1342 docker.go:533] Images already preloaded, skipping extraction
I0803 11:11:14.611215    1342 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0803 11:11:14.639114    1342 docker.go:602] Got preloaded images: -- stdout --
redis:latest
mongo:5.0
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
k8s.gcr.io/kube-apiserver:v1.24.1
k8s.gcr.io/kube-controller-manager:v1.24.1
k8s.gcr.io/kube-scheduler:v1.24.1
k8s.gcr.io/kube-proxy:v1.24.1
k8s.gcr.io/etcd:3.5.3-0
k8s.gcr.io/pause:3.7
k8s.gcr.io/coredns/coredns:v1.8.6
k8s.gcr.io/pause:3.6
gcr.io/k8s-minikube/storage-provisioner:v5
k8s.gcr.io/echoserver:1.10

-- /stdout --
I0803 11:11:14.639131    1342 cache_images.go:84] Images are preloaded, skipping loading
I0803 11:11:14.639199    1342 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0803 11:11:14.677607    1342 cni.go:95] Creating CNI manager for ""
I0803 11:11:14.677620    1342 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0803 11:11:14.677643    1342 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I0803 11:11:14.677663    1342 kubeadm.go:158] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.64.2 APIServerPort:8443 KubernetesVersion:v1.24.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.64.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NoTaintMaster:true NodeIP:192.168.64.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[]}
I0803 11:11:14.677828    1342 kubeadm.go:162] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.64.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: /var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.64.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.64.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.24.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0803 11:11:14.677927    1342 kubeadm.go:961] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.24.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime=remote --container-runtime-endpoint=/var/run/cri-dockerd.sock --hostname-override=minikube --image-service-endpoint=/var/run/cri-dockerd.sock --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.64.2 --runtime-request-timeout=15m

[Install]
 config:
{KubernetesVersion:v1.24.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I0803 11:11:14.678003    1342 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.24.1
I0803 11:11:14.690768    1342 binaries.go:44] Found k8s binaries, skipping transfer
I0803 11:11:14.690854    1342 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0803 11:11:14.701516    1342 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (470 bytes)
I0803 11:11:14.721690    1342 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0803 11:11:14.740606    1342 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2030 bytes)
I0803 11:11:14.763537    1342 ssh_runner.go:195] Run: grep 192.168.64.2	control-plane.minikube.internal$ /etc/hosts
I0803 11:11:14.768125    1342 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.64.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0803 11:11:14.781771    1342 certs.go:54] Setting up /Users/xiaolinsong/.minikube/profiles/minikube for IP: 192.168.64.2
I0803 11:11:14.781963    1342 certs.go:182] skipping minikubeCA CA generation: /Users/xiaolinsong/.minikube/ca.key
I0803 11:11:14.782349    1342 certs.go:182] skipping proxyClientCA CA generation: /Users/xiaolinsong/.minikube/proxy-client-ca.key
I0803 11:11:14.782490    1342 certs.go:298] skipping minikube-user signed cert generation: /Users/xiaolinsong/.minikube/profiles/minikube/client.key
I0803 11:11:14.783194    1342 certs.go:298] skipping minikube signed cert generation: /Users/xiaolinsong/.minikube/profiles/minikube/apiserver.key.a30f3483
I0803 11:11:14.783576    1342 certs.go:298] skipping aggregator signed cert generation: /Users/xiaolinsong/.minikube/profiles/minikube/proxy-client.key
I0803 11:11:14.783901    1342 certs.go:388] found cert: /Users/xiaolinsong/.minikube/certs/Users/xiaolinsong/.minikube/certs/ca-key.pem (1675 bytes)
I0803 11:11:14.783954    1342 certs.go:388] found cert: /Users/xiaolinsong/.minikube/certs/Users/xiaolinsong/.minikube/certs/ca.pem (1090 bytes)
I0803 11:11:14.784001    1342 certs.go:388] found cert: /Users/xiaolinsong/.minikube/certs/Users/xiaolinsong/.minikube/certs/cert.pem (1135 bytes)
I0803 11:11:14.784043    1342 certs.go:388] found cert: /Users/xiaolinsong/.minikube/certs/Users/xiaolinsong/.minikube/certs/key.pem (1679 bytes)
I0803 11:11:14.784725    1342 ssh_runner.go:362] scp /Users/xiaolinsong/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I0803 11:11:14.815232    1342 ssh_runner.go:362] scp /Users/xiaolinsong/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0803 11:11:14.843226    1342 ssh_runner.go:362] scp /Users/xiaolinsong/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0803 11:11:14.871497    1342 ssh_runner.go:362] scp /Users/xiaolinsong/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0803 11:11:14.900618    1342 ssh_runner.go:362] scp /Users/xiaolinsong/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0803 11:11:14.928784    1342 ssh_runner.go:362] scp /Users/xiaolinsong/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0803 11:11:14.956935    1342 ssh_runner.go:362] scp /Users/xiaolinsong/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0803 11:11:14.987812    1342 ssh_runner.go:362] scp /Users/xiaolinsong/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0803 11:11:15.024895    1342 ssh_runner.go:362] scp /Users/xiaolinsong/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0803 11:11:15.052812    1342 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (740 bytes)
I0803 11:11:15.074889    1342 ssh_runner.go:195] Run: openssl version
I0803 11:11:15.081416    1342 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0803 11:11:15.094349    1342 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0803 11:11:15.100046    1342 certs.go:431] hashing: -rw-r--r-- 1 root root 1111 Jul 29 14:16 /usr/share/ca-certificates/minikubeCA.pem
I0803 11:11:15.100123    1342 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0803 11:11:15.106317    1342 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0803 11:11:15.117976    1342 kubeadm.go:395] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO:https://storage.googleapis.com/minikube/iso/minikube-v1.26.0-amd64.iso KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.32@sha256:9190bd2393eae887316c97a74370b7d5dad8f0b2ef91ac2662bc36f7ef8e0b95 Memory:4000 CPUs:2 DiskSize:20000 VMDriver: Driver:hyperkit HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.24.1 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin: FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.64.2 Port:8443 KubernetesVersion:v1.24.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath:}
I0803 11:11:15.118123    1342 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0803 11:11:15.153932    1342 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0803 11:11:15.167674    1342 kubeadm.go:410] found existing configuration files, will attempt cluster restart
I0803 11:11:15.167694    1342 kubeadm.go:626] restartCluster start
I0803 11:11:15.167774    1342 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0803 11:11:15.178651    1342 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0803 11:11:15.179589    1342 kubeconfig.go:92] found "minikube" server: "https://192.168.64.2:8443"
I0803 11:11:15.185470    1342 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0803 11:11:15.194302    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:15.194365    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:15.206626    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:15.407939    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:15.408115    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:15.423634    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:15.606787    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:15.606861    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:15.621134    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:15.806761    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:15.806975    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:15.819197    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:16.006927    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:16.007070    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:16.022678    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:16.207168    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:16.207292    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:16.222450    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:16.406996    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:16.407189    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:16.424218    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:16.606813    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:16.606901    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:16.620412    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:16.807225    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:16.807329    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:16.821048    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:17.006826    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:17.006938    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:17.021130    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:17.206866    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:17.206992    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:17.224485    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:17.407785    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:17.407864    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:17.421627    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:17.606875    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:17.607039    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:17.620809    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:17.806904    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:17.807003    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:17.822263    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:18.006964    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:18.007115    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:18.021328    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:18.207233    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:18.207361    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:18.220746    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:18.220756    1342 api_server.go:165] Checking apiserver status ...
I0803 11:11:18.220815    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0803 11:11:18.234856    1342 api_server.go:169] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I0803 11:11:18.234868    1342 kubeadm.go:601] needs reconfigure: apiserver error: timed out waiting for the condition
I0803 11:11:18.234872    1342 kubeadm.go:1092] stopping kube-system containers ...
I0803 11:11:18.234922    1342 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0803 11:11:18.272130    1342 docker.go:434] Stopping containers: [19f9083e6d2f f214b83c4c51 c4b1b5bffb43 5673d3abba0e a9ef8c2cfe0b adf897ba11f1 a7cf2fa7a2f1 27f17247b21b ccadd8d07d29 697e25444a54 155bcdb24933 80efa5bcee70 c7af3053a8ad fe8964dc2212 f9ced6851abb 570a55401b6f c9f68e92f4f1 e1fbca25faf7 7d64912a1896 e14d8e438b2a 75d8a70c3cc5 c919f5084c12 86ba887c6eec 545c87743002 57b5953a300a 66c0a51f9676 650ef31b31bd]
I0803 11:11:18.272211    1342 ssh_runner.go:195] Run: docker stop 19f9083e6d2f f214b83c4c51 c4b1b5bffb43 5673d3abba0e a9ef8c2cfe0b adf897ba11f1 a7cf2fa7a2f1 27f17247b21b ccadd8d07d29 697e25444a54 155bcdb24933 80efa5bcee70 c7af3053a8ad fe8964dc2212 f9ced6851abb 570a55401b6f c9f68e92f4f1 e1fbca25faf7 7d64912a1896 e14d8e438b2a 75d8a70c3cc5 c919f5084c12 86ba887c6eec 545c87743002 57b5953a300a 66c0a51f9676 650ef31b31bd
I0803 11:11:18.311370    1342 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I0803 11:11:18.328685    1342 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0803 11:11:18.338841    1342 kubeadm.go:152] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0803 11:11:18.338909    1342 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0803 11:11:18.349158    1342 kubeadm.go:703] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I0803 11:11:18.349168    1342 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.1:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I0803 11:11:18.520548    1342 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.1:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I0803 11:11:19.866222    1342 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.1:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml": (1.345654262s)
I0803 11:11:19.866241    1342 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.1:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I0803 11:11:20.365704    1342 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.1:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I0803 11:11:20.439716    1342 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.1:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I0803 11:11:20.507819    1342 api_server.go:51] waiting for apiserver process to appear ...
I0803 11:11:20.507882    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0803 11:11:21.020405    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0803 11:11:21.520447    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0803 11:11:22.020932    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0803 11:11:22.520998    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0803 11:11:23.020309    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0803 11:11:23.520388    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0803 11:11:23.543449    1342 api_server.go:71] duration metric: took 3.035628338s to wait for apiserver process to appear ...
I0803 11:11:23.543482    1342 api_server.go:87] waiting for apiserver healthz status ...
I0803 11:11:23.543505    1342 api_server.go:240] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I0803 11:11:28.544049    1342 api_server.go:256] stopped: https://192.168.64.2:8443/healthz: Get "https://192.168.64.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I0803 11:11:29.044274    1342 api_server.go:240] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I0803 11:11:29.125753    1342 api_server.go:266] https://192.168.64.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0803 11:11:29.125768    1342 api_server.go:102] status: https://192.168.64.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0803 11:11:29.544587    1342 api_server.go:240] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I0803 11:11:29.554422    1342 api_server.go:266] https://192.168.64.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0803 11:11:29.554447    1342 api_server.go:102] status: https://192.168.64.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0803 11:11:30.044245    1342 api_server.go:240] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I0803 11:11:30.052788    1342 api_server.go:266] https://192.168.64.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0803 11:11:30.052811    1342 api_server.go:102] status: https://192.168.64.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0803 11:11:30.544428    1342 api_server.go:240] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I0803 11:11:30.561908    1342 api_server.go:266] https://192.168.64.2:8443/healthz returned 200:
ok
I0803 11:11:30.582525    1342 api_server.go:140] control plane version: v1.24.1
I0803 11:11:30.582556    1342 api_server.go:130] duration metric: took 7.039068706s to wait for apiserver health ...
I0803 11:11:30.582563    1342 cni.go:95] Creating CNI manager for ""
I0803 11:11:30.582570    1342 cni.go:169] CNI unnecessary in this configuration, recommending no CNI
I0803 11:11:30.582583    1342 system_pods.go:43] waiting for kube-system pods to appear ...
I0803 11:11:30.602703    1342 system_pods.go:59] 7 kube-system pods found
I0803 11:11:30.602723    1342 system_pods.go:61] "coredns-6d4b75cb6d-kptxw" [46850c4e-87a2-497b-b0bf-76e44ced9a6c] Running
I0803 11:11:30.602727    1342 system_pods.go:61] "etcd-minikube" [68552f8f-accd-4832-8a7f-4d90d23b950d] Running
I0803 11:11:30.602733    1342 system_pods.go:61] "kube-apiserver-minikube" [54510af4-194a-4b45-8957-bec3ddf98676] Running
I0803 11:11:30.602740    1342 system_pods.go:61] "kube-controller-manager-minikube" [7c45bd29-e108-4bbf-ae7d-167d44b34ccc] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0803 11:11:30.602745    1342 system_pods.go:61] "kube-proxy-czg8d" [deb97180-4ae7-4d63-86f2-d6b472a6fddf] Running
I0803 11:11:30.602751    1342 system_pods.go:61] "kube-scheduler-minikube" [8ed9b746-7493-4021-bec7-dacb1c64d38d] Running
I0803 11:11:30.602754    1342 system_pods.go:61] "storage-provisioner" [35e76fbb-e32f-4805-94e1-799d316f2c32] Running
I0803 11:11:30.602759    1342 system_pods.go:74] duration metric: took 20.171939ms to wait for pod list to return data ...
I0803 11:11:30.602763    1342 node_conditions.go:102] verifying NodePressure condition ...
I0803 11:11:30.617585    1342 node_conditions.go:122] node storage ephemeral capacity is 17784752Ki
I0803 11:11:30.617609    1342 node_conditions.go:123] node cpu capacity is 2
I0803 11:11:30.617619    1342 node_conditions.go:105] duration metric: took 14.85201ms to run NodePressure ...
I0803 11:11:30.617634    1342 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.24.1:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I0803 11:11:31.240740    1342 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0803 11:11:31.282619    1342 ops.go:34] apiserver oom_adj: -16
I0803 11:11:31.282628    1342 kubeadm.go:630] restartCluster took 16.114928979s
I0803 11:11:31.282633    1342 kubeadm.go:397] StartCluster complete in 16.164668754s
I0803 11:11:31.282642    1342 settings.go:142] acquiring lock: {Name:mk91767f3f13bd8a48c74048c33c45837352ba8f Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0803 11:11:31.282878    1342 settings.go:150] Updating kubeconfig:  /Users/xiaolinsong/.kube/config
I0803 11:11:31.283453    1342 lock.go:35] WriteFile acquiring /Users/xiaolinsong/.kube/config: {Name:mkce45bc5a915cab313038fc7dbb098dc8dd5044 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0803 11:11:31.292334    1342 kapi.go:244] deployment "coredns" in namespace "kube-system" and context "minikube" rescaled to 1
I0803 11:11:31.292376    1342 start.go:208] Will wait 6m0s for node &{Name: IP:192.168.64.2 Port:8443 KubernetesVersion:v1.24.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0803 11:11:31.292404    1342 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0803 11:11:31.305361    1342 out.go:177] üîé  Verifying Kubernetes components...
I0803 11:11:31.292441    1342 addons.go:412] enableAddons start: toEnable=map[ambassador:false auto-pause:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false], additional=[]
I0803 11:11:31.292718    1342 config.go:178] Loaded profile config "minikube": Driver=hyperkit, ContainerRuntime=docker, KubernetesVersion=v1.24.1
I0803 11:11:31.325759    1342 addons.go:65] Setting storage-provisioner=true in profile "minikube"
I0803 11:11:31.325757    1342 addons.go:65] Setting dashboard=true in profile "minikube"
I0803 11:11:31.325764    1342 addons.go:65] Setting default-storageclass=true in profile "minikube"
I0803 11:11:31.325772    1342 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I0803 11:11:31.325784    1342 addons.go:153] Setting addon storage-provisioner=true in "minikube"
I0803 11:11:31.325789    1342 addons.go:153] Setting addon dashboard=true in "minikube"
I0803 11:11:31.325793    1342 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W0803 11:11:31.325802    1342 addons.go:162] addon storage-provisioner should already be in state true
W0803 11:11:31.325802    1342 addons.go:162] addon dashboard should already be in state true
I0803 11:11:31.326529    1342 main.go:134] libmachine: Found binary path at /Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit
I0803 11:11:31.326572    1342 main.go:134] libmachine: Launching plugin server for driver hyperkit
I0803 11:11:31.326599    1342 host.go:66] Checking if "minikube" exists ...
I0803 11:11:31.326609    1342 host.go:66] Checking if "minikube" exists ...
I0803 11:11:31.329550    1342 main.go:134] libmachine: Found binary path at /Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit
I0803 11:11:31.329675    1342 main.go:134] libmachine: Launching plugin server for driver hyperkit
I0803 11:11:31.329717    1342 main.go:134] libmachine: Found binary path at /Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit
I0803 11:11:31.329843    1342 main.go:134] libmachine: Launching plugin server for driver hyperkit
I0803 11:11:31.345129    1342 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:49419
I0803 11:11:31.345381    1342 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:49420
I0803 11:11:31.346046    1342 main.go:134] libmachine: () Calling .GetVersion
I0803 11:11:31.346079    1342 main.go:134] libmachine: () Calling .GetVersion
I0803 11:11:31.346769    1342 main.go:134] libmachine: Using API Version  1
I0803 11:11:31.346785    1342 main.go:134] libmachine: () Calling .SetConfigRaw
I0803 11:11:31.347096    1342 main.go:134] libmachine: Using API Version  1
I0803 11:11:31.347157    1342 main.go:134] libmachine: () Calling .SetConfigRaw
I0803 11:11:31.347281    1342 main.go:134] libmachine: () Calling .GetMachineName
I0803 11:11:31.347509    1342 main.go:134] libmachine: (minikube) Calling .GetState
I0803 11:11:31.347741    1342 main.go:134] libmachine: (minikube) DBG | exe=/Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0803 11:11:31.347859    1342 main.go:134] libmachine: () Calling .GetMachineName
I0803 11:11:31.348009    1342 main.go:134] libmachine: (minikube) DBG | hyperkit pid from json: 1359
I0803 11:11:31.348596    1342 main.go:134] libmachine: Found binary path at /Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit
I0803 11:11:31.348649    1342 main.go:134] libmachine: Launching plugin server for driver hyperkit
I0803 11:11:31.352224    1342 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:49423
I0803 11:11:31.353033    1342 main.go:134] libmachine: () Calling .GetVersion
I0803 11:11:31.353690    1342 main.go:134] libmachine: Using API Version  1
I0803 11:11:31.353709    1342 main.go:134] libmachine: () Calling .SetConfigRaw
I0803 11:11:31.354182    1342 main.go:134] libmachine: () Calling .GetMachineName
I0803 11:11:31.354901    1342 main.go:134] libmachine: Found binary path at /Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit
I0803 11:11:31.355012    1342 main.go:134] libmachine: Launching plugin server for driver hyperkit
I0803 11:11:31.363554    1342 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:49425
I0803 11:11:31.364493    1342 main.go:134] libmachine: () Calling .GetVersion
I0803 11:11:31.365472    1342 main.go:134] libmachine: Using API Version  1
I0803 11:11:31.365549    1342 main.go:134] libmachine: () Calling .SetConfigRaw
I0803 11:11:31.365904    1342 main.go:134] libmachine: () Calling .GetMachineName
I0803 11:11:31.366106    1342 main.go:134] libmachine: (minikube) Calling .GetState
I0803 11:11:31.366289    1342 main.go:134] libmachine: (minikube) DBG | exe=/Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0803 11:11:31.366435    1342 main.go:134] libmachine: (minikube) DBG | hyperkit pid from json: 1359
I0803 11:11:31.368304    1342 main.go:134] libmachine: (minikube) Calling .DriverName
I0803 11:11:31.377038    1342 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0803 11:11:31.368710    1342 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:49427
I0803 11:11:31.377750    1342 main.go:134] libmachine: () Calling .GetVersion
I0803 11:11:31.378935    1342 addons.go:153] Setting addon default-storageclass=true in "minikube"
W0803 11:11:31.387944    1342 addons.go:162] addon default-storageclass should already be in state true
I0803 11:11:31.387978    1342 host.go:66] Checking if "minikube" exists ...
I0803 11:11:31.388022    1342 addons.go:345] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0803 11:11:31.388033    1342 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0803 11:11:31.388075    1342 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I0803 11:11:31.388392    1342 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I0803 11:11:31.388663    1342 main.go:134] libmachine: Using API Version  1
I0803 11:11:31.388681    1342 main.go:134] libmachine: () Calling .SetConfigRaw
I0803 11:11:31.388681    1342 main.go:134] libmachine: Found binary path at /Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit
I0803 11:11:31.388717    1342 main.go:134] libmachine: Launching plugin server for driver hyperkit
I0803 11:11:31.388770    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:31.389032    1342 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I0803 11:11:31.389903    1342 main.go:134] libmachine: () Calling .GetMachineName
I0803 11:11:31.389960    1342 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/xiaolinsong/.minikube/machines/minikube/id_rsa Username:docker}
I0803 11:11:31.390492    1342 main.go:134] libmachine: (minikube) Calling .GetState
I0803 11:11:31.391549    1342 main.go:134] libmachine: (minikube) DBG | exe=/Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0803 11:11:31.391831    1342 main.go:134] libmachine: (minikube) DBG | hyperkit pid from json: 1359
I0803 11:11:31.393228    1342 main.go:134] libmachine: (minikube) Calling .DriverName
I0803 11:11:31.402741    1342 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:49430
I0803 11:11:31.407599    1342 out.go:177]     ‚ñ™ Using image kubernetesui/dashboard:v2.6.0
I0803 11:11:31.408493    1342 main.go:134] libmachine: () Calling .GetVersion
I0803 11:11:31.419215    1342 main.go:134] libmachine: Using API Version  1
I0803 11:11:31.433143    1342 out.go:177]     ‚ñ™ Using image kubernetesui/metrics-scraper:v1.0.8
I0803 11:11:31.433199    1342 main.go:134] libmachine: () Calling .SetConfigRaw
I0803 11:11:31.455829    1342 addons.go:345] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0803 11:11:31.455851    1342 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0803 11:11:31.455875    1342 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I0803 11:11:31.456156    1342 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I0803 11:11:31.456380    1342 main.go:134] libmachine: () Calling .GetMachineName
I0803 11:11:31.456417    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:31.456593    1342 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I0803 11:11:31.456750    1342 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/xiaolinsong/.minikube/machines/minikube/id_rsa Username:docker}
I0803 11:11:31.457081    1342 main.go:134] libmachine: Found binary path at /Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit
I0803 11:11:31.457207    1342 main.go:134] libmachine: Launching plugin server for driver hyperkit
I0803 11:11:31.469636    1342 main.go:134] libmachine: Plugin server listening at address 127.0.0.1:49433
I0803 11:11:31.470290    1342 main.go:134] libmachine: () Calling .GetVersion
I0803 11:11:31.470945    1342 main.go:134] libmachine: Using API Version  1
I0803 11:11:31.470963    1342 main.go:134] libmachine: () Calling .SetConfigRaw
I0803 11:11:31.471381    1342 main.go:134] libmachine: () Calling .GetMachineName
I0803 11:11:31.471564    1342 main.go:134] libmachine: (minikube) Calling .GetState
I0803 11:11:31.471771    1342 main.go:134] libmachine: (minikube) DBG | exe=/Users/xiaolinsong/.minikube/bin/docker-machine-driver-hyperkit uid=0
I0803 11:11:31.471862    1342 main.go:134] libmachine: (minikube) DBG | hyperkit pid from json: 1359
I0803 11:11:31.473576    1342 main.go:134] libmachine: (minikube) Calling .DriverName
I0803 11:11:31.474000    1342 addons.go:345] installing /etc/kubernetes/addons/storageclass.yaml
I0803 11:11:31.474009    1342 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0803 11:11:31.474023    1342 main.go:134] libmachine: (minikube) Calling .GetSSHHostname
I0803 11:11:31.474243    1342 main.go:134] libmachine: (minikube) Calling .GetSSHPort
I0803 11:11:31.474418    1342 main.go:134] libmachine: (minikube) Calling .GetSSHKeyPath
I0803 11:11:31.474565    1342 main.go:134] libmachine: (minikube) Calling .GetSSHUsername
I0803 11:11:31.474729    1342 sshutil.go:53] new ssh client: &{IP:192.168.64.2 Port:22 SSHKeyPath:/Users/xiaolinsong/.minikube/machines/minikube/id_rsa Username:docker}
I0803 11:11:31.700086    1342 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0803 11:11:31.787899    1342 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0803 11:11:31.802486    1342 addons.go:345] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0803 11:11:31.802496    1342 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0803 11:11:31.958610    1342 addons.go:345] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0803 11:11:31.958622    1342 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0803 11:11:32.208020    1342 addons.go:345] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0803 11:11:32.208031    1342 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0803 11:11:32.369294    1342 addons.go:345] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0803 11:11:32.369305    1342 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4278 bytes)
I0803 11:11:32.657046    1342 addons.go:345] installing /etc/kubernetes/addons/dashboard-role.yaml
I0803 11:11:32.657059    1342 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0803 11:11:32.698282    1342 ssh_runner.go:235] Completed: sudo systemctl is-active --quiet service kubelet: (1.372487542s)
I0803 11:11:32.698328    1342 api_server.go:51] waiting for apiserver process to appear ...
I0803 11:11:32.698409    1342 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0803 11:11:32.698866    1342 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.24.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml": (1.393509344s)
I0803 11:11:32.698932    1342 start.go:786] CoreDNS already contains "host.minikube.internal" host record, skipping...
I0803 11:11:32.785865    1342 addons.go:345] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0803 11:11:32.785879    1342 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0803 11:11:32.856887    1342 addons.go:345] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0803 11:11:32.856899    1342 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0803 11:11:32.917368    1342 addons.go:345] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0803 11:11:32.917380    1342 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0803 11:11:32.988723    1342 addons.go:345] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0803 11:11:32.988736    1342 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0803 11:11:33.035471    1342 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.1/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0803 11:11:35.077025    1342 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.376903687s)
I0803 11:11:35.077058    1342 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (3.289143309s)
I0803 11:11:35.077069    1342 main.go:134] libmachine: Making call to close driver server
I0803 11:11:35.077082    1342 main.go:134] libmachine: Making call to close driver server
I0803 11:11:35.077085    1342 main.go:134] libmachine: (minikube) Calling .Close
I0803 11:11:35.077091    1342 main.go:134] libmachine: (minikube) Calling .Close
I0803 11:11:35.077104    1342 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (2.378681765s)
I0803 11:11:35.077118    1342 api_server.go:71] duration metric: took 3.784722469s to wait for apiserver process to appear ...
I0803 11:11:35.077172    1342 api_server.go:87] waiting for apiserver healthz status ...
I0803 11:11:35.077222    1342 api_server.go:240] Checking apiserver healthz at https://192.168.64.2:8443/healthz ...
I0803 11:11:35.077475    1342 main.go:134] libmachine: (minikube) DBG | Closing plugin on server side
I0803 11:11:35.077478    1342 main.go:134] libmachine: Successfully made call to close driver server
I0803 11:11:35.077489    1342 main.go:134] libmachine: Making call to close connection to plugin binary
I0803 11:11:35.077502    1342 main.go:134] libmachine: Successfully made call to close driver server
I0803 11:11:35.077510    1342 main.go:134] libmachine: Making call to close driver server
I0803 11:11:35.077514    1342 main.go:134] libmachine: (minikube) DBG | Closing plugin on server side
I0803 11:11:35.077517    1342 main.go:134] libmachine: Making call to close connection to plugin binary
I0803 11:11:35.077521    1342 main.go:134] libmachine: (minikube) Calling .Close
I0803 11:11:35.077529    1342 main.go:134] libmachine: Making call to close driver server
I0803 11:11:35.077552    1342 main.go:134] libmachine: (minikube) Calling .Close
I0803 11:11:35.077848    1342 main.go:134] libmachine: (minikube) DBG | Closing plugin on server side
I0803 11:11:35.077860    1342 main.go:134] libmachine: Successfully made call to close driver server
I0803 11:11:35.077890    1342 main.go:134] libmachine: Making call to close connection to plugin binary
I0803 11:11:35.077890    1342 main.go:134] libmachine: Successfully made call to close driver server
I0803 11:11:35.077899    1342 main.go:134] libmachine: Making call to close connection to plugin binary
I0803 11:11:35.077908    1342 main.go:134] libmachine: Making call to close driver server
I0803 11:11:35.077918    1342 main.go:134] libmachine: (minikube) Calling .Close
I0803 11:11:35.078315    1342 main.go:134] libmachine: (minikube) DBG | Closing plugin on server side
I0803 11:11:35.078362    1342 main.go:134] libmachine: Successfully made call to close driver server
I0803 11:11:35.078371    1342 main.go:134] libmachine: Making call to close connection to plugin binary
I0803 11:11:35.099816    1342 api_server.go:266] https://192.168.64.2:8443/healthz returned 200:
ok
I0803 11:11:35.101136    1342 api_server.go:140] control plane version: v1.24.1
I0803 11:11:35.101145    1342 api_server.go:130] duration metric: took 23.957008ms to wait for apiserver health ...
I0803 11:11:35.101150    1342 system_pods.go:43] waiting for kube-system pods to appear ...
I0803 11:11:35.114651    1342 system_pods.go:59] 7 kube-system pods found
I0803 11:11:35.114666    1342 system_pods.go:61] "coredns-6d4b75cb6d-kptxw" [46850c4e-87a2-497b-b0bf-76e44ced9a6c] Running
I0803 11:11:35.114675    1342 system_pods.go:61] "etcd-minikube" [68552f8f-accd-4832-8a7f-4d90d23b950d] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0803 11:11:35.114686    1342 system_pods.go:61] "kube-apiserver-minikube" [54510af4-194a-4b45-8957-bec3ddf98676] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0803 11:11:35.114691    1342 system_pods.go:61] "kube-controller-manager-minikube" [7c45bd29-e108-4bbf-ae7d-167d44b34ccc] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0803 11:11:35.114698    1342 system_pods.go:61] "kube-proxy-czg8d" [deb97180-4ae7-4d63-86f2-d6b472a6fddf] Running
I0803 11:11:35.114703    1342 system_pods.go:61] "kube-scheduler-minikube" [8ed9b746-7493-4021-bec7-dacb1c64d38d] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0803 11:11:35.114707    1342 system_pods.go:61] "storage-provisioner" [35e76fbb-e32f-4805-94e1-799d316f2c32] Running
I0803 11:11:35.114711    1342 system_pods.go:74] duration metric: took 13.557637ms to wait for pod list to return data ...
I0803 11:11:35.114719    1342 kubeadm.go:572] duration metric: took 3.822323408s to wait for : map[apiserver:true system_pods:true] ...
I0803 11:11:35.114728    1342 node_conditions.go:102] verifying NodePressure condition ...
I0803 11:11:35.118786    1342 node_conditions.go:122] node storage ephemeral capacity is 17784752Ki
I0803 11:11:35.118798    1342 node_conditions.go:123] node cpu capacity is 2
I0803 11:11:35.118804    1342 node_conditions.go:105] duration metric: took 4.073488ms to run NodePressure ...
I0803 11:11:35.118814    1342 start.go:213] waiting for startup goroutines ...
I0803 11:11:35.215801    1342 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.24.1/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (2.180297007s)
I0803 11:11:35.215840    1342 main.go:134] libmachine: Making call to close driver server
I0803 11:11:35.215850    1342 main.go:134] libmachine: (minikube) Calling .Close
I0803 11:11:35.216131    1342 main.go:134] libmachine: Successfully made call to close driver server
I0803 11:11:35.216142    1342 main.go:134] libmachine: Making call to close connection to plugin binary
I0803 11:11:35.216152    1342 main.go:134] libmachine: Making call to close driver server
I0803 11:11:35.216162    1342 main.go:134] libmachine: (minikube) Calling .Close
I0803 11:11:35.216166    1342 main.go:134] libmachine: (minikube) DBG | Closing plugin on server side
I0803 11:11:35.216377    1342 main.go:134] libmachine: Successfully made call to close driver server
I0803 11:11:35.216389    1342 main.go:134] libmachine: Making call to close connection to plugin binary
I0803 11:11:35.216402    1342 main.go:134] libmachine: (minikube) DBG | Closing plugin on server side
I0803 11:11:35.229317    1342 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass, dashboard
I0803 11:11:35.250768    1342 addons.go:414] enableAddons completed in 3.958338569s
I0803 11:11:35.315739    1342 start.go:503] kubectl: 1.24.2, cluster: 1.24.1 (minor skew: 0)
I0803 11:11:35.326467    1342 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* -- Journal begins at Wed 2022-08-03 09:11:01 UTC, ends at Wed 2022-08-03 09:33:59 UTC. --
Aug 03 09:11:32 minikube dockerd[926]: time="2022-08-03T09:11:32.755161740Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 03 09:11:32 minikube dockerd[926]: time="2022-08-03T09:11:32.755296590Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 03 09:11:32 minikube dockerd[926]: time="2022-08-03T09:11:32.755717734Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/7e80885b0e877b878a1dc45b549fae3082cedb382360a2837703520c06b05a1c pid=2304 runtime=io.containerd.runc.v2
Aug 03 09:11:32 minikube dockerd[926]: time="2022-08-03T09:11:32.762409420Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 03 09:11:32 minikube dockerd[926]: time="2022-08-03T09:11:32.766379141Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 03 09:11:32 minikube dockerd[926]: time="2022-08-03T09:11:32.766641706Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 03 09:11:32 minikube dockerd[926]: time="2022-08-03T09:11:32.819904788Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/a3cc5bd4bff9a9bea862496b9013055f9dc541e783aa4500527d7c09dfc5602e pid=2289 runtime=io.containerd.runc.v2
Aug 03 09:11:32 minikube dockerd[926]: time="2022-08-03T09:11:32.895119534Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 03 09:11:32 minikube dockerd[926]: time="2022-08-03T09:11:32.901862006Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 03 09:11:32 minikube dockerd[926]: time="2022-08-03T09:11:32.902124612Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 03 09:11:32 minikube dockerd[926]: time="2022-08-03T09:11:32.959935360Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/8d87e78f25168305b4e94074b0510977b04ff6fe6f911344c421363fb037d1a5 pid=2331 runtime=io.containerd.runc.v2
Aug 03 09:11:32 minikube dockerd[926]: time="2022-08-03T09:11:32.992482459Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 03 09:11:32 minikube dockerd[926]: time="2022-08-03T09:11:32.992795770Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 03 09:11:32 minikube dockerd[926]: time="2022-08-03T09:11:32.992983271Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 03 09:11:33 minikube dockerd[926]: time="2022-08-03T09:11:33.032545078Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/552dc6be339a22ad838560d7e50c3a38f2a1b40a2162dc1126e5ccb3377df79f pid=2364 runtime=io.containerd.runc.v2
Aug 03 09:11:33 minikube dockerd[926]: time="2022-08-03T09:11:33.386775658Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 03 09:11:33 minikube dockerd[926]: time="2022-08-03T09:11:33.386995760Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 03 09:11:33 minikube dockerd[926]: time="2022-08-03T09:11:33.387010426Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 03 09:11:33 minikube dockerd[926]: time="2022-08-03T09:11:33.387991473Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/cabaf417a3fedc093ce3a930e7ceffc72b5038138cb0e84bb3c43c2f7f64aa91 pid=2394 runtime=io.containerd.runc.v2
Aug 03 09:11:35 minikube dockerd[926]: time="2022-08-03T09:11:35.241098244Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 03 09:11:35 minikube dockerd[926]: time="2022-08-03T09:11:35.241703201Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 03 09:11:35 minikube dockerd[926]: time="2022-08-03T09:11:35.242020279Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 03 09:11:35 minikube dockerd[926]: time="2022-08-03T09:11:35.243679467Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/991161d536bc95c3021f256044bd17447f43338844423f8d234921063eb9f283 pid=2555 runtime=io.containerd.runc.v2
Aug 03 09:11:35 minikube dockerd[926]: time="2022-08-03T09:11:35.447352948Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 03 09:11:35 minikube dockerd[926]: time="2022-08-03T09:11:35.447454966Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 03 09:11:35 minikube dockerd[926]: time="2022-08-03T09:11:35.447469376Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 03 09:11:35 minikube dockerd[926]: time="2022-08-03T09:11:35.448853394Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/be2cee8cf06e622703e87d5d53b7da644d8fd9d945179aeb8947e91c2136825f pid=2586 runtime=io.containerd.runc.v2
Aug 03 09:11:35 minikube dockerd[926]: time="2022-08-03T09:11:35.566631970Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 03 09:11:35 minikube dockerd[926]: time="2022-08-03T09:11:35.566808633Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 03 09:11:35 minikube dockerd[926]: time="2022-08-03T09:11:35.566833292Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 03 09:11:35 minikube dockerd[926]: time="2022-08-03T09:11:35.568639668Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/7c94a77ad61da4b64edc78e95d6ac06256ffca1ff2599756962e60aff62a8449 pid=2618 runtime=io.containerd.runc.v2
Aug 03 09:11:36 minikube dockerd[926]: time="2022-08-03T09:11:36.457955686Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 03 09:11:36 minikube dockerd[926]: time="2022-08-03T09:11:36.460307597Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 03 09:11:36 minikube dockerd[926]: time="2022-08-03T09:11:36.460356667Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 03 09:11:36 minikube dockerd[926]: time="2022-08-03T09:11:36.462427350Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/2e917fbff27846b10a48166b102c03d966a9dd77e8f7bbacedfc67a5c90ad5fa pid=2713 runtime=io.containerd.runc.v2
Aug 03 09:11:37 minikube dockerd[926]: time="2022-08-03T09:11:37.304374385Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 03 09:11:37 minikube dockerd[926]: time="2022-08-03T09:11:37.304488573Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 03 09:11:37 minikube dockerd[926]: time="2022-08-03T09:11:37.304504005Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 03 09:11:37 minikube dockerd[926]: time="2022-08-03T09:11:37.305833598Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/901a9ea602c79fa8921936d923eca40efad83da41eddd2547fab9582381e3a78 pid=2852 runtime=io.containerd.runc.v2
Aug 03 09:11:37 minikube dockerd[926]: time="2022-08-03T09:11:37.408338546Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 03 09:11:37 minikube dockerd[926]: time="2022-08-03T09:11:37.409527517Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 03 09:11:37 minikube dockerd[926]: time="2022-08-03T09:11:37.409549751Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 03 09:11:37 minikube dockerd[926]: time="2022-08-03T09:11:37.414365768Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/cb2136acb120bb03a6c7f5fc4dc1c166974b7d2d257f41d7628781ffb45a3bfa pid=2878 runtime=io.containerd.runc.v2
Aug 03 09:11:37 minikube dockerd[926]: time="2022-08-03T09:11:37.458772653Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 03 09:11:37 minikube dockerd[926]: time="2022-08-03T09:11:37.458903128Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 03 09:11:37 minikube dockerd[926]: time="2022-08-03T09:11:37.458916081Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 03 09:11:37 minikube dockerd[926]: time="2022-08-03T09:11:37.459115899Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/1ef7c267c0ce83d9db29c73c0f87e8610bdb7f7d53547f398bbc4f40afc9fda2 pid=2912 runtime=io.containerd.runc.v2
Aug 03 09:11:46 minikube dockerd[926]: time="2022-08-03T09:11:46.500162077Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 03 09:11:46 minikube dockerd[926]: time="2022-08-03T09:11:46.500232204Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 03 09:11:46 minikube dockerd[926]: time="2022-08-03T09:11:46.500244766Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 03 09:11:46 minikube dockerd[926]: time="2022-08-03T09:11:46.500699875Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/7a60bcb5ed52e655191fcbcfdb2fecde7fdb999140766740d68567c6060211d6 pid=3371 runtime=io.containerd.runc.v2
Aug 03 09:12:06 minikube dockerd[920]: time="2022-08-03T09:12:06.256963336Z" level=info msg="ignoring event" container=7c94a77ad61da4b64edc78e95d6ac06256ffca1ff2599756962e60aff62a8449 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 03 09:12:06 minikube dockerd[926]: time="2022-08-03T09:12:06.258523019Z" level=info msg="shim disconnected" id=7c94a77ad61da4b64edc78e95d6ac06256ffca1ff2599756962e60aff62a8449
Aug 03 09:12:06 minikube dockerd[926]: time="2022-08-03T09:12:06.258689290Z" level=warning msg="cleaning up after shim disconnected" id=7c94a77ad61da4b64edc78e95d6ac06256ffca1ff2599756962e60aff62a8449 namespace=moby
Aug 03 09:12:06 minikube dockerd[926]: time="2022-08-03T09:12:06.258705321Z" level=info msg="cleaning up dead shim"
Aug 03 09:12:06 minikube dockerd[926]: time="2022-08-03T09:12:06.275905530Z" level=warning msg="cleanup warnings time=\"2022-08-03T09:12:06Z\" level=info msg=\"starting signal loop\" namespace=moby pid=3628 runtime=io.containerd.runc.v2\n"
Aug 03 09:12:18 minikube dockerd[926]: time="2022-08-03T09:12:18.835134452Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Aug 03 09:12:18 minikube dockerd[926]: time="2022-08-03T09:12:18.835423604Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Aug 03 09:12:18 minikube dockerd[926]: time="2022-08-03T09:12:18.835441013Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Aug 03 09:12:18 minikube dockerd[926]: time="2022-08-03T09:12:18.836849129Z" level=info msg="starting signal loop" namespace=moby path=/run/docker/containerd/daemon/io.containerd.runtime.v2.task/moby/4ad9b04f7a095f04b6c81b65b7b19c42c13828551d5f67a225dd0e13d30f4a1a pid=3858 runtime=io.containerd.runc.v2

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                        ATTEMPT             POD ID
4ad9b04f7a095       6e38f40d628db                                                                   21 minutes ago      Running             storage-provisioner         13                  8d87e78f25168
7a60bcb5ed52e       redis@sha256:dbb6d3b2577d3f0deb8c2511f1dc9244153e2256b1dcd7309eedb340842d3664   22 minutes ago      Running             redis                       2                   025b31a093d97
1ef7c267c0ce8       c8b57c4bf7e3a                                                                   22 minutes ago      Running             mongodb                     1                   cabaf417a3fed
cb2136acb120b       1042d9e0d8fcc                                                                   22 minutes ago      Running             kubernetes-dashboard        5                   9363542f4ace5
901a9ea602c79       115053965e86b                                                                   22 minutes ago      Running             dashboard-metrics-scraper   5                   7e80885b0e877
2e917fbff2784       a4ca41631cc7a                                                                   22 minutes ago      Running             coredns                     7                   552dc6be339a2
7c94a77ad61da       6e38f40d628db                                                                   22 minutes ago      Exited              storage-provisioner         12                  8d87e78f25168
991161d536bc9       365ec60129c54                                                                   22 minutes ago      Running             echoserver                  2                   9e0ab4a3ba626
be2cee8cf06e6       beb86f5d8e6cd                                                                   22 minutes ago      Running             kube-proxy                  7                   a3cc5bd4bff9a
b912906c6ab93       aebe758cef4cd                                                                   22 minutes ago      Running             etcd                        7                   e4c83730081cb
e123bb3445dc2       18688a72645c5                                                                   22 minutes ago      Running             kube-scheduler              7                   381a8cf8e4dfa
6dfccea4a874d       b4ea7e648530d                                                                   22 minutes ago      Running             kube-controller-manager     7                   56bb7ded7fd6a
350c4ff923a7e       e9f4b425f9192                                                                   22 minutes ago      Running             kube-apiserver              7                   f2bacc465bd99
7ea12746ebf38       mongo@sha256:82302b06360729842acd27ab8a91c90e244f17e464fcfd366b7427af652c5559   2 days ago          Exited              mongodb                     0                   2b63986615722
87a71e754d3f4       redis@sha256:ed8cba11c09451dbb3495f15951e4afb4f1ba72a4a13e135c6da06c6346e0333   2 days ago          Exited              redis                       1                   98ffc33683449
7d9976dad7837       115053965e86b                                                                   2 days ago          Exited              dashboard-metrics-scraper   4                   a9f75a9d3886a
10cd3c49d4217       1042d9e0d8fcc                                                                   2 days ago          Exited              kubernetes-dashboard        4                   3e35505476f6b
f214b83c4c517       a4ca41631cc7a                                                                   2 days ago          Exited              coredns                     6                   adf897ba11f13
ea929809286f2       365ec60129c54                                                                   2 days ago          Exited              echoserver                  1                   c2538a8379f7b
c4b1b5bffb43a       beb86f5d8e6cd                                                                   2 days ago          Exited              kube-proxy                  6                   a9ef8c2cfe0bb
27f17247b21bc       18688a72645c5                                                                   2 days ago          Exited              kube-scheduler              6                   c7af3053a8ad0
ccadd8d07d29f       aebe758cef4cd                                                                   2 days ago          Exited              etcd                        6                   80efa5bcee703
697e25444a54a       b4ea7e648530d                                                                   2 days ago          Exited              kube-controller-manager     6                   fe8964dc2212c
155bcdb249339       e9f4b425f9192                                                                   2 days ago          Exited              kube-apiserver              6                   f9ced6851abbf

* 
* ==> coredns [2e917fbff278] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = 08e2b174e0f0a30a2e82df9c995f4a34
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> coredns [f214b83c4c51] <==
* [INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration MD5 = 08e2b174e0f0a30a2e82df9c995f4a34
CoreDNS-1.8.6
linux/amd64, go1.17.1, 13a9191
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f4b412861bb746be73053c9f6d2895f12cf78565
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2022_07_29T16_16_33_0700
                    minikube.k8s.io/version=v1.26.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 29 Jul 2022 14:16:31 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 03 Aug 2022 09:33:57 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 03 Aug 2022 09:32:25 +0000   Fri, 29 Jul 2022 14:16:31 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 03 Aug 2022 09:32:25 +0000   Fri, 29 Jul 2022 14:16:31 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 03 Aug 2022 09:32:25 +0000   Fri, 29 Jul 2022 14:16:31 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 03 Aug 2022 09:32:25 +0000   Fri, 29 Jul 2022 14:16:44 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.64.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             3914676Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  17784752Ki
  hugepages-2Mi:      0
  memory:             3914676Ki
  pods:               110
System Info:
  Machine ID:                 52194ab01cde431fbffa2a5c4985bc27
  System UUID:                eae711ed-0000-0000-8d0e-f40f241f4ec8
  Boot ID:                    957c0949-34d5-4c27-9cde-ee5dd62215d9
  Kernel Version:             5.10.57
  OS Image:                   Buildroot 2021.02.12
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://20.10.16
  Kubelet Version:            v1.24.1
  Kube-Proxy Version:         v1.24.1
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     hello-minikube-66d5c9996d-jpbcf               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d19h
  default                     mongo-deployment-7d4d5c9f6c-lvcvp             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         2d21h
  default                     redis-794bfd9659-mpdr2                        0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         3d19h
  kube-system                 coredns-6d4b75cb6d-kptxw                      100m (5%!)(MISSING)     0 (0%!)(MISSING)      70Mi (1%!)(MISSING)        170Mi (4%!)(MISSING)     4d19h
  kube-system                 etcd-minikube                                 100m (5%!)(MISSING)     0 (0%!)(MISSING)      100Mi (2%!)(MISSING)       0 (0%!)(MISSING)         4d19h
  kube-system                 kube-apiserver-minikube                       250m (12%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d19h
  kube-system                 kube-controller-manager-minikube              200m (10%!)(MISSING)    0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d19h
  kube-system                 kube-proxy-czg8d                              0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d19h
  kube-system                 kube-scheduler-minikube                       100m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d19h
  kube-system                 storage-provisioner                           0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d19h
  kubernetes-dashboard        dashboard-metrics-scraper-78dbd9dbf5-bg74g    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d18h
  kubernetes-dashboard        kubernetes-dashboard-5fd5574d9f-s2wmh         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4d18h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%!)(MISSING)  0 (0%!)(MISSING)
  memory             170Mi (4%!)(MISSING)  170Mi (4%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 22m                kube-proxy       
  Normal  Starting                 22m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  22m (x8 over 22m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    22m (x8 over 22m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     22m (x7 over 22m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  22m                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           22m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [Aug 3 09:10] ERROR: earlyprintk= earlyser already used
[  +0.000000] You have booted with nomodeset. This means your GPU drivers are DISABLED
[  +0.000001] Any video related functionality will be severely degraded, and you may not even be able to suspend the system properly
[  +0.000000] Unless you actually understand what nomodeset does, you should reboot without enabling it
[  +0.188305] ACPI BIOS Warning (bug): Incorrect checksum in table [DSDT] - 0xBE, should be 0x1B (20200925/tbprint-173)
[  +6.594457] ACPI Error: Could not enable RealTimeClock event (20200925/evxfevnt-182)
[  +0.000003] ACPI Warning: Could not enable fixed event - RealTimeClock (4) (20200925/evxface-618)
[  +0.013680] platform regulatory.0: Direct firmware load for regulatory.db failed with error -2
[Aug 3 09:11] systemd-fstab-generator[125]: Ignoring "noauto" for root device
[  +0.059609] systemd[1]: systemd-journald.service: unit configures an IP firewall, but the local system does not support BPF/cgroup firewalling.
[  +0.000002] systemd[1]: (This warning is only shown for the first unit using IP firewalling.)
[  +2.712846] NFSD: Using /var/lib/nfs/v4recovery as the NFSv4 state recovery directory
[  +0.000007] NFSD: unable to find recovery directory /var/lib/nfs/v4recovery
[  +0.000001] NFSD: Unable to initialize client recovery tracking! (-2)
[  +5.728986] systemd-fstab-generator[520]: Ignoring "noauto" for root device
[  +0.141744] systemd-fstab-generator[531]: Ignoring "noauto" for root device
[  +2.558241] systemd-fstab-generator[889]: Ignoring "noauto" for root device
[  +0.148461] systemd-fstab-generator[900]: Ignoring "noauto" for root device
[  +0.160355] systemd-fstab-generator[911]: Ignoring "noauto" for root device
[  +1.699974] systemd-fstab-generator[1073]: Ignoring "noauto" for root device
[  +0.135451] systemd-fstab-generator[1084]: Ignoring "noauto" for root device
[  +6.085816] systemd-fstab-generator[1325]: Ignoring "noauto" for root device
[  +0.273028] kauditd_printk_skb: 67 callbacks suppressed
[  +9.768078] kauditd_printk_skb: 7 callbacks suppressed
[ +11.885475] kauditd_printk_skb: 8 callbacks suppressed
[Aug 3 09:14] hrtimer: interrupt took 26296411 ns

* 
* ==> etcd [b912906c6ab9] <==
* {"level":"info","ts":"2022-08-03T09:11:24.050Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.64.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--initial-advertise-peer-urls=https://192.168.64.2:2380","--initial-cluster=minikube=https://192.168.64.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.64.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.64.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2022-08-03T09:11:24.054Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"info","ts":"2022-08-03T09:11:24.054Z","caller":"embed/etcd.go:131","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.64.2:2380"]}
{"level":"info","ts":"2022-08-03T09:11:24.054Z","caller":"embed/etcd.go:479","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-08-03T09:11:24.060Z","caller":"embed/etcd.go:139","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.64.2:2379"]}
{"level":"info","ts":"2022-08-03T09:11:24.060Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.3","git-sha":"0452feec7","go-version":"go1.16.15","go-os":"linux","go-arch":"amd64","max-cpu-set":2,"max-cpu-available":2,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.64.2:2380"],"listen-peer-urls":["https://192.168.64.2:2380"],"advertise-client-urls":["https://192.168.64.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.64.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-size-bytes":2147483648,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2022-08-03T09:11:24.104Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"42.765154ms"}
{"level":"info","ts":"2022-08-03T09:11:24.677Z","caller":"etcdserver/server.go:508","msg":"recovered v2 store from snapshot","snapshot-index":20002,"snapshot-size":"9.3 kB"}
{"level":"info","ts":"2022-08-03T09:11:24.677Z","caller":"etcdserver/server.go:521","msg":"recovered v3 backend from snapshot","backend-size-bytes":3346432,"backend-size":"3.3 MB","backend-size-in-use-bytes":1417216,"backend-size-in-use":"1.4 MB"}
{"level":"info","ts":"2022-08-03T09:11:24.979Z","caller":"etcdserver/raft.go:483","msg":"restarting local member","cluster-id":"bde34cecb57cc33f","local-member-id":"77ee71d7bb7b0f2","commit-index":26905}
{"level":"info","ts":"2022-08-03T09:11:24.980Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"77ee71d7bb7b0f2 switched to configuration voters=(540123119146742002)"}
{"level":"info","ts":"2022-08-03T09:11:24.980Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"77ee71d7bb7b0f2 became follower at term 8"}
{"level":"info","ts":"2022-08-03T09:11:24.980Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft 77ee71d7bb7b0f2 [peers: [77ee71d7bb7b0f2], term: 8, commit: 26905, applied: 20002, lastindex: 26905, lastterm: 8]"}
{"level":"info","ts":"2022-08-03T09:11:24.980Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2022-08-03T09:11:24.980Z","caller":"membership/cluster.go:278","msg":"recovered/added member from store","cluster-id":"bde34cecb57cc33f","local-member-id":"77ee71d7bb7b0f2","recovered-remote-peer-id":"77ee71d7bb7b0f2","recovered-remote-peer-urls":["https://192.168.64.2:2380"]}
{"level":"info","ts":"2022-08-03T09:11:24.980Z","caller":"membership/cluster.go:287","msg":"set cluster version from store","cluster-version":"3.5"}
{"level":"warn","ts":"2022-08-03T09:11:24.982Z","caller":"auth/store.go:1220","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2022-08-03T09:11:24.984Z","caller":"mvcc/kvstore.go:345","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":20606}
{"level":"info","ts":"2022-08-03T09:11:24.989Z","caller":"mvcc/kvstore.go:415","msg":"kvstore restored","current-rev":20998}
{"level":"info","ts":"2022-08-03T09:11:24.994Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2022-08-03T09:11:25.002Z","caller":"etcdserver/corrupt.go:46","msg":"starting initial corruption check","local-member-id":"77ee71d7bb7b0f2","timeout":"7s"}
{"level":"info","ts":"2022-08-03T09:11:25.005Z","caller":"etcdserver/corrupt.go:116","msg":"initial corruption checking passed; no corruption","local-member-id":"77ee71d7bb7b0f2"}
{"level":"info","ts":"2022-08-03T09:11:25.005Z","caller":"etcdserver/server.go:842","msg":"starting etcd server","local-member-id":"77ee71d7bb7b0f2","local-server-version":"3.5.3","cluster-id":"bde34cecb57cc33f","cluster-version":"3.5"}
{"level":"info","ts":"2022-08-03T09:11:25.006Z","caller":"etcdserver/server.go:736","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"77ee71d7bb7b0f2","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2022-08-03T09:11:25.013Z","caller":"embed/etcd.go:688","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2022-08-03T09:11:25.014Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"77ee71d7bb7b0f2","initial-advertise-peer-urls":["https://192.168.64.2:2380"],"listen-peer-urls":["https://192.168.64.2:2380"],"advertise-client-urls":["https://192.168.64.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.64.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2022-08-03T09:11:25.014Z","caller":"embed/etcd.go:763","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2022-08-03T09:11:25.016Z","caller":"embed/etcd.go:581","msg":"serving peer traffic","address":"192.168.64.2:2380"}
{"level":"info","ts":"2022-08-03T09:11:25.016Z","caller":"embed/etcd.go:553","msg":"cmux::serve","address":"192.168.64.2:2380"}
{"level":"info","ts":"2022-08-03T09:11:25.480Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"77ee71d7bb7b0f2 is starting a new election at term 8"}
{"level":"info","ts":"2022-08-03T09:11:25.481Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"77ee71d7bb7b0f2 became pre-candidate at term 8"}
{"level":"info","ts":"2022-08-03T09:11:25.481Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"77ee71d7bb7b0f2 received MsgPreVoteResp from 77ee71d7bb7b0f2 at term 8"}
{"level":"info","ts":"2022-08-03T09:11:25.481Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"77ee71d7bb7b0f2 became candidate at term 9"}
{"level":"info","ts":"2022-08-03T09:11:25.481Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"77ee71d7bb7b0f2 received MsgVoteResp from 77ee71d7bb7b0f2 at term 9"}
{"level":"info","ts":"2022-08-03T09:11:25.481Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"77ee71d7bb7b0f2 became leader at term 9"}
{"level":"info","ts":"2022-08-03T09:11:25.481Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 77ee71d7bb7b0f2 elected leader 77ee71d7bb7b0f2 at term 9"}
{"level":"info","ts":"2022-08-03T09:11:25.495Z","caller":"etcdserver/server.go:2042","msg":"published local member to cluster through raft","local-member-id":"77ee71d7bb7b0f2","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.64.2:2379]}","request-path":"/0/members/77ee71d7bb7b0f2/attributes","cluster-id":"bde34cecb57cc33f","publish-timeout":"7s"}
{"level":"info","ts":"2022-08-03T09:11:25.495Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-08-03T09:11:25.495Z","caller":"embed/serve.go:98","msg":"ready to serve client requests"}
{"level":"info","ts":"2022-08-03T09:11:25.497Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"192.168.64.2:2379"}
{"level":"info","ts":"2022-08-03T09:11:25.497Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2022-08-03T09:11:25.497Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2022-08-03T09:11:25.498Z","caller":"embed/serve.go:188","msg":"serving client traffic securely","address":"127.0.0.1:2379"}
{"level":"info","ts":"2022-08-03T09:21:25.535Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21310}
{"level":"info","ts":"2022-08-03T09:21:25.554Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":21310,"took":"18.754133ms"}
{"level":"info","ts":"2022-08-03T09:26:25.546Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21521}
{"level":"info","ts":"2022-08-03T09:26:25.546Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":21521,"took":"487.354¬µs"}
{"level":"info","ts":"2022-08-03T09:31:25.556Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21730}
{"level":"info","ts":"2022-08-03T09:31:25.557Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":21730,"took":"573.824¬µs"}

* 
* ==> etcd [ccadd8d07d29] <==
* {"level":"info","ts":"2022-07-31T13:29:03.541Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":14725,"took":"875.276¬µs"}
{"level":"info","ts":"2022-07-31T13:34:03.569Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":14935}
{"level":"info","ts":"2022-07-31T13:34:03.570Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":14935,"took":"929.174¬µs"}
{"level":"info","ts":"2022-07-31T13:39:03.580Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15145}
{"level":"info","ts":"2022-07-31T13:39:03.581Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":15145,"took":"389.813¬µs"}
{"level":"info","ts":"2022-07-31T13:44:03.587Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15355}
{"level":"info","ts":"2022-07-31T13:44:03.588Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":15355,"took":"432.901¬µs"}
{"level":"info","ts":"2022-07-31T13:46:03.516Z","caller":"etcdserver/server.go:1383","msg":"triggering snapshot","local-member-id":"77ee71d7bb7b0f2","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000}
{"level":"info","ts":"2022-07-31T13:46:03.520Z","caller":"etcdserver/server.go:2394","msg":"saved snapshot","snapshot-index":20002}
{"level":"info","ts":"2022-07-31T13:46:03.520Z","caller":"etcdserver/server.go:2424","msg":"compacted Raft logs","compact-index":15002}
{"level":"info","ts":"2022-07-31T13:49:03.598Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15566}
{"level":"info","ts":"2022-07-31T13:49:03.598Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":15566,"took":"448.781¬µs"}
{"level":"info","ts":"2022-07-31T13:54:03.609Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15775}
{"level":"info","ts":"2022-07-31T13:54:03.610Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":15775,"took":"1.107571ms"}
{"level":"info","ts":"2022-07-31T13:59:03.619Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15984}
{"level":"info","ts":"2022-07-31T13:59:03.620Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":15984,"took":"521.696¬µs"}
{"level":"info","ts":"2022-07-31T14:04:03.629Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16195}
{"level":"info","ts":"2022-07-31T14:04:03.630Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":16195,"took":"543.935¬µs"}
{"level":"info","ts":"2022-07-31T14:09:03.639Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16405}
{"level":"info","ts":"2022-07-31T14:09:03.640Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":16405,"took":"582.275¬µs"}
{"level":"info","ts":"2022-07-31T14:14:03.650Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16614}
{"level":"info","ts":"2022-07-31T14:14:03.652Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":16614,"took":"528.56¬µs"}
{"level":"info","ts":"2022-07-31T14:19:03.659Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16825}
{"level":"info","ts":"2022-07-31T14:19:03.660Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":16825,"took":"527.209¬µs"}
{"level":"info","ts":"2022-07-31T14:24:03.669Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17035}
{"level":"info","ts":"2022-07-31T14:24:03.670Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":17035,"took":"537.735¬µs"}
{"level":"info","ts":"2022-07-31T14:29:03.678Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17244}
{"level":"info","ts":"2022-07-31T14:29:03.679Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":17244,"took":"914.595¬µs"}
{"level":"info","ts":"2022-07-31T14:34:03.690Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17455}
{"level":"info","ts":"2022-07-31T14:34:03.691Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":17455,"took":"691.338¬µs"}
{"level":"info","ts":"2022-07-31T14:39:03.696Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17664}
{"level":"info","ts":"2022-07-31T14:39:03.697Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":17664,"took":"1.094284ms"}
{"level":"info","ts":"2022-07-31T14:44:03.707Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17875}
{"level":"info","ts":"2022-07-31T14:44:03.708Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":17875,"took":"562.098¬µs"}
{"level":"info","ts":"2022-07-31T14:49:03.717Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18085}
{"level":"info","ts":"2022-07-31T14:49:03.718Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":18085,"took":"904.889¬µs"}
{"level":"info","ts":"2022-07-31T14:54:03.727Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18294}
{"level":"info","ts":"2022-07-31T14:54:03.728Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":18294,"took":"519.147¬µs"}
{"level":"info","ts":"2022-07-31T14:59:03.737Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18505}
{"level":"info","ts":"2022-07-31T14:59:03.738Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":18505,"took":"510.998¬µs"}
{"level":"info","ts":"2022-07-31T15:04:03.746Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18715}
{"level":"info","ts":"2022-07-31T15:04:03.747Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":18715,"took":"529.971¬µs"}
{"level":"info","ts":"2022-07-31T15:09:03.757Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18925}
{"level":"info","ts":"2022-07-31T15:09:03.758Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":18925,"took":"519.393¬µs"}
{"level":"info","ts":"2022-07-31T15:14:03.764Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19135}
{"level":"info","ts":"2022-07-31T15:14:03.766Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":19135,"took":"436.132¬µs"}
{"level":"info","ts":"2022-07-31T15:19:03.776Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19345}
{"level":"info","ts":"2022-07-31T15:19:03.777Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":19345,"took":"384.434¬µs"}
{"level":"info","ts":"2022-07-31T15:24:03.787Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19555}
{"level":"info","ts":"2022-07-31T15:24:03.788Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":19555,"took":"559.878¬µs"}
{"level":"info","ts":"2022-07-31T15:29:03.799Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19766}
{"level":"info","ts":"2022-07-31T15:29:03.800Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":19766,"took":"664.292¬µs"}
{"level":"info","ts":"2022-07-31T15:34:03.810Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19975}
{"level":"info","ts":"2022-07-31T15:34:03.812Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":19975,"took":"572.254¬µs"}
{"level":"info","ts":"2022-07-31T15:39:03.821Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20186}
{"level":"info","ts":"2022-07-31T15:39:03.822Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":20186,"took":"531.358¬µs"}
{"level":"info","ts":"2022-07-31T15:44:03.829Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20396}
{"level":"info","ts":"2022-07-31T15:44:03.829Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":20396,"took":"429.478¬µs"}
{"level":"info","ts":"2022-07-31T15:49:03.835Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20606}
{"level":"info","ts":"2022-07-31T15:49:03.835Z","caller":"mvcc/kvstore_compaction.go:57","msg":"finished scheduled compaction","compact-revision":20606,"took":"622.667¬µs"}

* 
* ==> kernel <==
*  09:34:00 up 23 min,  0 users,  load average: 0.74, 0.51, 0.40
Linux minikube 5.10.57 #1 SMP Thu Jun 16 23:36:20 UTC 2022 x86_64 GNU/Linux
PRETTY_NAME="Buildroot 2021.02.12"

* 
* ==> kube-apiserver [155bcdb24933] <==
* I0731 11:59:06.730429       1 available_controller.go:491] Starting AvailableConditionController
I0731 11:59:06.730645       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0731 11:59:06.730874       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0731 11:59:06.731008       1 shared_informer.go:255] Waiting for caches to sync for crd-autoregister
I0731 11:59:06.745488       1 controller.go:85] Starting OpenAPI controller
I0731 11:59:06.745556       1 controller.go:85] Starting OpenAPI V3 controller
I0731 11:59:06.745616       1 naming_controller.go:291] Starting NamingConditionController
I0731 11:59:06.746451       1 establishing_controller.go:76] Starting EstablishingController
I0731 11:59:06.750838       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0731 11:59:06.751015       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0731 11:59:06.751227       1 crd_finalizer.go:266] Starting CRDFinalizer
I0731 11:59:06.753816       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0731 11:59:06.754027       1 shared_informer.go:255] Waiting for caches to sync for cluster_authentication_trust_controller
I0731 11:59:06.782274       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0731 11:59:06.801743       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0731 11:59:06.899990       1 cache.go:39] Caches are synced for autoregister controller
I0731 11:59:06.900479       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0731 11:59:06.914404       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0731 11:59:06.917281       1 shared_informer.go:262] Caches are synced for node_authorizer
I0731 11:59:06.930757       1 apf_controller.go:322] Running API Priority and Fairness config worker
I0731 11:59:06.931165       1 shared_informer.go:262] Caches are synced for crd-autoregister
I0731 11:59:06.960739       1 shared_informer.go:262] Caches are synced for cluster_authentication_trust_controller
I0731 11:59:06.965143       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I0731 11:59:07.227532       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0731 11:59:07.735418       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0731 11:59:08.660916       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I0731 11:59:08.676048       1 controller.go:611] quota admission added evaluator for: deployments.apps
I0731 11:59:08.749458       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I0731 11:59:08.778554       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0731 11:59:08.789107       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0731 11:59:13.289474       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io
E0731 11:59:14.082193       1 storage.go:461] Address {172.17.0.3  0xc00e1277a0 0xc003e490a0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [] vs 172.17.0.3 (kubernetes-dashboard/dashboard-metrics-scraper-78dbd9dbf5-bg74g))
E0731 11:59:14.082516       1 storage.go:471] Failed to find a valid address, skipping subset: &{[{172.17.0.3  0xc00e1277a0 0xc003e490a0}] [] [{ 8000 TCP <nil>}]}
I0731 11:59:19.921520       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0731 11:59:19.967924       1 controller.go:611] quota admission added evaluator for: endpoints
I0731 12:02:18.336777       1 controller.go:611] quota admission added evaluator for: replicasets.apps
I0731 12:02:18.350965       1 alloc.go:327] "allocated clusterIPs" service="default/mongo-service" clusterIPs=map[IPv4:10.111.0.4]
W0731 12:11:14.554788       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 12:22:50.890200       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 12:31:31.886489       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 12:45:58.917744       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 13:11:08.090913       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 13:19:19.307145       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 13:34:31.361921       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
I0731 13:44:12.495083       1 trace.go:205] Trace[88095898]: "Get" url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,audit-id:d4263c2d-30bf-47d9-bec7-4796fa031102,client:192.168.64.2,accept:application/json, */*,protocol:HTTP/2.0 (31-Jul-2022 13:44:11.487) (total time: 1007ms):
Trace[88095898]: ---"About to write a response" 1007ms (13:44:12.494)
Trace[88095898]: [1.007864595s] [1.007864595s] END
W0731 13:52:03.843827       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 13:59:40.828931       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 14:13:30.106513       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 14:20:17.306762       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 14:34:50.308077       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 14:50:52.979719       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 15:04:25.223308       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 15:16:30.672377       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 15:31:19.834447       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 15:39:24.370024       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
W0731 15:49:04.593718       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted
E0731 17:21:12.135009       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"
E0731 17:21:12.180908       1 authentication.go:63] "Unable to authenticate the request" err="[invalid bearer token, service account token has expired]"

* 
* ==> kube-apiserver [350c4ff923a7] <==
* W0803 09:11:26.580599       1 genericapiserver.go:557] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
W0803 09:11:26.585773       1 genericapiserver.go:557] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
W0803 09:11:26.594190       1 genericapiserver.go:557] Skipping API flowcontrol.apiserver.k8s.io/v1alpha1 because it has no resources.
W0803 09:11:26.599510       1 genericapiserver.go:557] Skipping API apps/v1beta2 because it has no resources.
W0803 09:11:26.599529       1 genericapiserver.go:557] Skipping API apps/v1beta1 because it has no resources.
W0803 09:11:26.601938       1 genericapiserver.go:557] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
I0803 09:11:26.607433       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0803 09:11:26.607452       1 plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,CertificateSubjectRestriction,ValidatingAdmissionWebhook,ResourceQuota.
W0803 09:11:26.641817       1 genericapiserver.go:557] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0803 09:11:29.034120       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0803 09:11:29.034949       1 secure_serving.go:210] Serving securely on [::]:8443
I0803 09:11:29.035086       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0803 09:11:29.050638       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0803 09:11:29.051300       1 controller.go:83] Starting OpenAPI AggregationController
I0803 09:11:29.055146       1 customresource_discovery_controller.go:209] Starting DiscoveryController
I0803 09:11:29.055301       1 apf_controller.go:317] Starting API Priority and Fairness config controller
I0803 09:11:29.055389       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0803 09:11:29.059402       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0803 09:11:29.059450       1 shared_informer.go:255] Waiting for caches to sync for cluster_authentication_trust_controller
I0803 09:11:29.059624       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0803 09:11:29.060027       1 available_controller.go:491] Starting AvailableConditionController
I0803 09:11:29.060043       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0803 09:11:29.060124       1 autoregister_controller.go:141] Starting autoregister controller
I0803 09:11:29.060162       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0803 09:11:29.050629       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0803 09:11:29.072176       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0803 09:11:29.072268       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0803 09:11:29.072886       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0803 09:11:29.081648       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0803 09:11:29.082569       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0803 09:11:29.082615       1 shared_informer.go:255] Waiting for caches to sync for crd-autoregister
I0803 09:11:29.106939       1 controller.go:85] Starting OpenAPI controller
I0803 09:11:29.107065       1 controller.go:85] Starting OpenAPI V3 controller
I0803 09:11:29.107465       1 naming_controller.go:291] Starting NamingConditionController
I0803 09:11:29.107621       1 establishing_controller.go:76] Starting EstablishingController
I0803 09:11:29.108216       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0803 09:11:29.108389       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0803 09:11:29.108532       1 crd_finalizer.go:266] Starting CRDFinalizer
I0803 09:11:29.182723       1 shared_informer.go:262] Caches are synced for crd-autoregister
E0803 09:11:29.209749       1 controller.go:169] Error removing old endpoints from kubernetes service: no master IPs were listed in storage, refusing to erase all endpoints for the kubernetes service
I0803 09:11:29.227749       1 shared_informer.go:262] Caches are synced for node_authorizer
I0803 09:11:29.255876       1 apf_controller.go:322] Running API Priority and Fairness config worker
I0803 09:11:29.259485       1 shared_informer.go:262] Caches are synced for cluster_authentication_trust_controller
I0803 09:11:29.261317       1 cache.go:39] Caches are synced for autoregister controller
I0803 09:11:29.263105       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0803 09:11:29.272345       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0803 09:11:29.285663       1 controller.go:611] quota admission added evaluator for: leases.coordination.k8s.io
I0803 09:11:29.488568       1 controller.go:132] OpenAPI AggregationController: action for item k8s_internal_local_delegation_chain_0000000000: Nothing (removed from the queue).
I0803 09:11:30.061378       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0803 09:11:31.070229       1 controller.go:611] quota admission added evaluator for: serviceaccounts
I0803 09:11:31.093993       1 controller.go:611] quota admission added evaluator for: deployments.apps
I0803 09:11:31.187060       1 controller.go:611] quota admission added evaluator for: daemonsets.apps
I0803 09:11:31.236356       1 controller.go:611] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0803 09:11:31.247692       1 controller.go:611] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0803 09:11:37.105640       1 controller.go:611] quota admission added evaluator for: events.events.k8s.io
E0803 09:11:38.154896       1 storage.go:461] Address {172.17.0.5  0xc00e8b1440 0xc002b6dea0} isn't valid (pod ip(s) doesn't match endpoint ip, skipping: [] vs 172.17.0.5 (kubernetes-dashboard/dashboard-metrics-scraper-78dbd9dbf5-bg74g))
E0803 09:11:38.155085       1 storage.go:471] Failed to find a valid address, skipping subset: &{[{172.17.0.5  0xc00e8b1440 0xc002b6dea0}] [] [{ 8000 TCP <nil>}]}
I0803 09:11:41.970646       1 controller.go:611] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0803 09:11:41.991961       1 controller.go:611] quota admission added evaluator for: endpoints
W0803 09:25:34.866820       1 watcher.go:229] watch chan error: etcdserver: mvcc: required revision has been compacted

* 
* ==> kube-controller-manager [697e25444a54] <==
* I0731 11:59:19.412041       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I0731 11:59:19.412078       1 graph_builder.go:289] GraphBuilder running
I0731 11:59:19.412091       1 controllermanager.go:593] Started "garbagecollector"
I0731 11:59:19.463130       1 controllermanager.go:593] Started "daemonset"
I0731 11:59:19.463587       1 daemon_controller.go:284] Starting daemon sets controller
I0731 11:59:19.464738       1 shared_informer.go:255] Waiting for caches to sync for daemon sets
I0731 11:59:19.483873       1 shared_informer.go:255] Waiting for caches to sync for resource quota
W0731 11:59:19.515587       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I0731 11:59:19.522985       1 shared_informer.go:262] Caches are synced for namespace
I0731 11:59:19.539024       1 shared_informer.go:262] Caches are synced for TTL after finished
I0731 11:59:19.540103       1 shared_informer.go:262] Caches are synced for node
I0731 11:59:19.540222       1 range_allocator.go:173] Starting range CIDR allocator
I0731 11:59:19.540280       1 shared_informer.go:255] Waiting for caches to sync for cidrallocator
I0731 11:59:19.540394       1 shared_informer.go:262] Caches are synced for cidrallocator
I0731 11:59:19.544618       1 shared_informer.go:262] Caches are synced for ReplicationController
I0731 11:59:19.546970       1 shared_informer.go:262] Caches are synced for endpoint_slice_mirroring
I0731 11:59:19.549634       1 shared_informer.go:262] Caches are synced for ReplicaSet
I0731 11:59:19.550084       1 shared_informer.go:262] Caches are synced for deployment
I0731 11:59:19.556591       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-serving
I0731 11:59:19.556733       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-client
I0731 11:59:19.556870       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0731 11:59:19.560791       1 shared_informer.go:262] Caches are synced for crt configmap
I0731 11:59:19.561164       1 shared_informer.go:262] Caches are synced for job
I0731 11:59:19.561672       1 shared_informer.go:262] Caches are synced for taint
I0731 11:59:19.561814       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-legacy-unknown
I0731 11:59:19.562155       1 node_lifecycle_controller.go:1399] Initializing eviction metric for zone: 
W0731 11:59:19.562641       1 node_lifecycle_controller.go:1014] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0731 11:59:19.563094       1 node_lifecycle_controller.go:1215] Controller detected that zone  is now in state Normal.
I0731 11:59:19.565075       1 shared_informer.go:262] Caches are synced for daemon sets
I0731 11:59:19.565170       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
I0731 11:59:19.565887       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0731 11:59:19.566891       1 shared_informer.go:262] Caches are synced for endpoint_slice
I0731 11:59:19.569778       1 shared_informer.go:262] Caches are synced for certificate-csrapproving
I0731 11:59:19.572996       1 shared_informer.go:262] Caches are synced for TTL
I0731 11:59:19.573600       1 shared_informer.go:262] Caches are synced for GC
I0731 11:59:19.571100       1 shared_informer.go:262] Caches are synced for PV protection
I0731 11:59:19.574745       1 shared_informer.go:262] Caches are synced for service account
I0731 11:59:19.581314       1 shared_informer.go:262] Caches are synced for bootstrap_signer
I0731 11:59:19.589655       1 shared_informer.go:262] Caches are synced for HPA
I0731 11:59:19.595565       1 shared_informer.go:262] Caches are synced for cronjob
I0731 11:59:19.598407       1 shared_informer.go:262] Caches are synced for endpoint
I0731 11:59:19.603789       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I0731 11:59:19.643967       1 shared_informer.go:262] Caches are synced for ClusterRoleAggregator
I0731 11:59:19.672933       1 shared_informer.go:262] Caches are synced for ephemeral
I0731 11:59:19.686229       1 shared_informer.go:262] Caches are synced for stateful set
I0731 11:59:19.696026       1 shared_informer.go:262] Caches are synced for attach detach
I0731 11:59:19.729440       1 shared_informer.go:262] Caches are synced for persistent volume
I0731 11:59:19.756475       1 shared_informer.go:262] Caches are synced for PVC protection
I0731 11:59:19.761136       1 shared_informer.go:262] Caches are synced for expand
I0731 11:59:19.776222       1 shared_informer.go:262] Caches are synced for disruption
I0731 11:59:19.776564       1 disruption.go:371] Sending events to api server.
I0731 11:59:19.784750       1 shared_informer.go:262] Caches are synced for resource quota
I0731 11:59:19.800951       1 shared_informer.go:262] Caches are synced for resource quota
I0731 11:59:20.205197       1 shared_informer.go:262] Caches are synced for garbage collector
I0731 11:59:20.212454       1 shared_informer.go:262] Caches are synced for garbage collector
I0731 11:59:20.212477       1 garbagecollector.go:158] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0731 12:02:18.343967       1 event.go:294] "Event occurred" object="default/mongo-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set mongo-deployment-7d4d5c9f6c to 1"
I0731 12:02:18.368632       1 event.go:294] "Event occurred" object="default/mongo-deployment-7d4d5c9f6c" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: mongo-deployment-7d4d5c9f6c-lvcvp"
E0731 17:21:12.137414       1 resource_quota_controller.go:413] failed to discover resources: Unauthorized
W0731 17:21:12.183188       1 garbagecollector.go:749] failed to discover preferred resources: Unauthorized

* 
* ==> kube-controller-manager [6dfccea4a874] <==
* I0803 09:11:41.708216       1 shared_informer.go:262] Caches are synced for token_cleaner
I0803 09:11:41.714532       1 controllermanager.go:593] Started "persistentvolume-binder"
I0803 09:11:41.714694       1 pv_controller_base.go:311] Starting persistent volume controller
I0803 09:11:41.714738       1 shared_informer.go:255] Waiting for caches to sync for persistent volume
I0803 09:11:41.717961       1 controllermanager.go:593] Started "replicationcontroller"
I0803 09:11:41.718262       1 replica_set.go:205] Starting replicationcontroller controller
I0803 09:11:41.718273       1 shared_informer.go:255] Waiting for caches to sync for ReplicationController
I0803 09:11:41.724579       1 controllermanager.go:593] Started "deployment"
I0803 09:11:41.727269       1 deployment_controller.go:153] "Starting controller" controller="deployment"
I0803 09:11:41.727864       1 shared_informer.go:255] Waiting for caches to sync for deployment
I0803 09:11:41.755739       1 shared_informer.go:255] Waiting for caches to sync for resource quota
I0803 09:11:41.800307       1 shared_informer.go:262] Caches are synced for TTL after finished
I0803 09:11:41.807365       1 shared_informer.go:262] Caches are synced for certificate-csrapproving
W0803 09:11:41.808141       1 actual_state_of_world.go:541] Failed to update statusUpdateNeeded field in actual state of world: Failed to set statusUpdateNeeded to needed true, because nodeName="minikube" does not exist
I0803 09:11:41.809451       1 shared_informer.go:262] Caches are synced for ClusterRoleAggregator
I0803 09:11:41.825013       1 shared_informer.go:262] Caches are synced for PV protection
I0803 09:11:41.835277       1 shared_informer.go:262] Caches are synced for node
I0803 09:11:41.835333       1 range_allocator.go:173] Starting range CIDR allocator
I0803 09:11:41.835339       1 shared_informer.go:255] Waiting for caches to sync for cidrallocator
I0803 09:11:41.835348       1 shared_informer.go:262] Caches are synced for cidrallocator
I0803 09:11:41.835448       1 shared_informer.go:262] Caches are synced for endpoint_slice_mirroring
I0803 09:11:41.836480       1 shared_informer.go:262] Caches are synced for service account
I0803 09:11:41.841390       1 shared_informer.go:255] Waiting for caches to sync for garbage collector
I0803 09:11:41.841952       1 shared_informer.go:262] Caches are synced for expand
I0803 09:11:41.857212       1 shared_informer.go:262] Caches are synced for PVC protection
I0803 09:11:41.857220       1 shared_informer.go:262] Caches are synced for disruption
I0803 09:11:41.857243       1 disruption.go:371] Sending events to api server.
I0803 09:11:41.858400       1 shared_informer.go:262] Caches are synced for stateful set
I0803 09:11:41.880852       1 shared_informer.go:262] Caches are synced for cronjob
I0803 09:11:41.881193       1 shared_informer.go:262] Caches are synced for ephemeral
I0803 09:11:41.883387       1 shared_informer.go:262] Caches are synced for job
I0803 09:11:41.884636       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-serving
I0803 09:11:41.885853       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kubelet-client
I0803 09:11:41.891686       1 shared_informer.go:262] Caches are synced for namespace
I0803 09:11:41.892754       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0803 09:11:41.892845       1 shared_informer.go:262] Caches are synced for certificate-csrsigning-legacy-unknown
I0803 09:11:41.896256       1 shared_informer.go:262] Caches are synced for GC
I0803 09:11:41.896273       1 shared_informer.go:262] Caches are synced for ReplicaSet
I0803 09:11:41.900914       1 shared_informer.go:262] Caches are synced for TTL
I0803 09:11:41.901160       1 shared_informer.go:262] Caches are synced for HPA
I0803 09:11:41.915018       1 shared_informer.go:262] Caches are synced for persistent volume
I0803 09:11:41.918416       1 shared_informer.go:262] Caches are synced for ReplicationController
I0803 09:11:41.928528       1 shared_informer.go:262] Caches are synced for deployment
I0803 09:11:41.931898       1 shared_informer.go:262] Caches are synced for endpoint_slice
I0803 09:11:41.940126       1 shared_informer.go:262] Caches are synced for attach detach
I0803 09:11:41.957461       1 shared_informer.go:262] Caches are synced for endpoint
I0803 09:11:41.957591       1 shared_informer.go:262] Caches are synced for taint
I0803 09:11:41.957641       1 node_lifecycle_controller.go:1399] Initializing eviction metric for zone: 
W0803 09:11:41.957752       1 node_lifecycle_controller.go:1014] Missing timestamp for Node minikube. Assuming now as a timestamp.
I0803 09:11:41.957841       1 node_lifecycle_controller.go:1215] Controller detected that zone  is now in state Normal.
I0803 09:11:41.959228       1 taint_manager.go:187] "Starting NoExecuteTaintManager"
I0803 09:11:41.959600       1 event.go:294] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I0803 09:11:41.970997       1 shared_informer.go:262] Caches are synced for daemon sets
I0803 09:11:42.029436       1 shared_informer.go:262] Caches are synced for crt configmap
I0803 09:11:42.039836       1 shared_informer.go:262] Caches are synced for resource quota
I0803 09:11:42.056403       1 shared_informer.go:262] Caches are synced for resource quota
I0803 09:11:42.066760       1 shared_informer.go:262] Caches are synced for bootstrap_signer
I0803 09:11:42.453780       1 shared_informer.go:262] Caches are synced for garbage collector
I0803 09:11:42.453825       1 garbagecollector.go:158] Garbage collector: all resource monitors have synced. Proceeding to collect garbage
I0803 09:11:42.541872       1 shared_informer.go:262] Caches are synced for garbage collector

* 
* ==> kube-proxy [be2cee8cf06e] <==
* I0803 09:11:36.902097       1 node.go:163] Successfully retrieved node IP: 192.168.64.2
I0803 09:11:36.902311       1 server_others.go:138] "Detected node IP" address="192.168.64.2"
I0803 09:11:36.902570       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0803 09:11:37.051106       1 server_others.go:199] "kube-proxy running in single-stack mode, this ipFamily is not supported" ipFamily=IPv6
I0803 09:11:37.051127       1 server_others.go:206] "Using iptables Proxier"
I0803 09:11:37.051150       1 proxier.go:259] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0803 09:11:37.052231       1 server.go:661] "Version info" version="v1.24.1"
I0803 09:11:37.052250       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0803 09:11:37.055289       1 config.go:317] "Starting service config controller"
I0803 09:11:37.055814       1 shared_informer.go:255] Waiting for caches to sync for service config
I0803 09:11:37.055856       1 config.go:226] "Starting endpoint slice config controller"
I0803 09:11:37.055863       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I0803 09:11:37.057459       1 config.go:444] "Starting node config controller"
I0803 09:11:37.057472       1 shared_informer.go:255] Waiting for caches to sync for node config
I0803 09:11:37.156260       1 shared_informer.go:262] Caches are synced for endpoint slice config
I0803 09:11:37.156308       1 shared_informer.go:262] Caches are synced for service config
I0803 09:11:37.158371       1 shared_informer.go:262] Caches are synced for node config

* 
* ==> kube-proxy [c4b1b5bffb43] <==
* I0731 11:59:13.136588       1 node.go:163] Successfully retrieved node IP: 192.168.64.2
I0731 11:59:13.136864       1 server_others.go:138] "Detected node IP" address="192.168.64.2"
I0731 11:59:13.137404       1 server_others.go:578] "Unknown proxy mode, assuming iptables proxy" proxyMode=""
I0731 11:59:13.268420       1 server_others.go:199] "kube-proxy running in single-stack mode, this ipFamily is not supported" ipFamily=IPv6
I0731 11:59:13.268533       1 server_others.go:206] "Using iptables Proxier"
I0731 11:59:13.268702       1 proxier.go:259] "Setting route_localnet=1, use nodePortAddresses to filter loopback addresses for NodePorts to skip it https://issues.k8s.io/90259"
I0731 11:59:13.272949       1 server.go:661] "Version info" version="v1.24.1"
I0731 11:59:13.273240       1 server.go:663] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0731 11:59:13.278950       1 config.go:317] "Starting service config controller"
I0731 11:59:13.279914       1 shared_informer.go:255] Waiting for caches to sync for service config
I0731 11:59:13.280375       1 config.go:444] "Starting node config controller"
I0731 11:59:13.286148       1 shared_informer.go:255] Waiting for caches to sync for node config
I0731 11:59:13.283469       1 config.go:226] "Starting endpoint slice config controller"
I0731 11:59:13.286946       1 shared_informer.go:255] Waiting for caches to sync for endpoint slice config
I0731 11:59:13.391057       1 shared_informer.go:262] Caches are synced for service config
I0731 11:59:13.397605       1 shared_informer.go:262] Caches are synced for node config
I0731 11:59:13.409044       1 shared_informer.go:262] Caches are synced for endpoint slice config

* 
* ==> kube-scheduler [27f17247b21b] <==
* I0731 11:59:03.982689       1 serving.go:348] Generated self-signed cert in-memory
W0731 11:59:06.815386       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0731 11:59:06.815535       1 authentication.go:346] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0731 11:59:06.815580       1 authentication.go:347] Continuing without authentication configuration. This may treat all requests as anonymous.
W0731 11:59:06.815724       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0731 11:59:06.916684       1 server.go:147] "Starting Kubernetes Scheduler" version="v1.24.1"
I0731 11:59:06.916708       1 server.go:149] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0731 11:59:06.928156       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0731 11:59:06.928246       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0731 11:59:06.928284       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0731 11:59:06.928305       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0731 11:59:07.029998       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [e123bb3445dc] <==
* I0803 09:11:25.055591       1 serving.go:348] Generated self-signed cert in-memory
W0803 09:11:29.131142       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0803 09:11:29.131528       1 authentication.go:346] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0803 09:11:29.131644       1 authentication.go:347] Continuing without authentication configuration. This may treat all requests as anonymous.
W0803 09:11:29.131879       1 authentication.go:348] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0803 09:11:29.251254       1 server.go:147] "Starting Kubernetes Scheduler" version="v1.24.1"
I0803 09:11:29.251282       1 server.go:149] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0803 09:11:29.257784       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0803 09:11:29.268299       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0803 09:11:29.263336       1 secure_serving.go:210] Serving securely on 127.0.0.1:10259
I0803 09:11:29.268256       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0803 09:11:29.369240       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kubelet <==
* -- Journal begins at Wed 2022-08-03 09:11:01 UTC, ends at Wed 2022-08-03 09:34:01 UTC. --
Aug 03 09:11:27 minikube kubelet[1331]: E0803 09:11:27.220965    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:27 minikube kubelet[1331]: E0803 09:11:27.321352    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:27 minikube kubelet[1331]: E0803 09:11:27.421692    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:27 minikube kubelet[1331]: E0803 09:11:27.522602    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:27 minikube kubelet[1331]: E0803 09:11:27.622854    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:27 minikube kubelet[1331]: E0803 09:11:27.723709    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:27 minikube kubelet[1331]: E0803 09:11:27.824795    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:27 minikube kubelet[1331]: E0803 09:11:27.925780    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:28 minikube kubelet[1331]: E0803 09:11:28.026356    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:28 minikube kubelet[1331]: E0803 09:11:28.127141    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:28 minikube kubelet[1331]: E0803 09:11:28.227254    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:28 minikube kubelet[1331]: E0803 09:11:28.327612    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:28 minikube kubelet[1331]: E0803 09:11:28.427772    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:28 minikube kubelet[1331]: E0803 09:11:28.528322    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:28 minikube kubelet[1331]: E0803 09:11:28.629297    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:28 minikube kubelet[1331]: E0803 09:11:28.730447    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:28 minikube kubelet[1331]: E0803 09:11:28.831565    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:28 minikube kubelet[1331]: E0803 09:11:28.932841    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:29 minikube kubelet[1331]: E0803 09:11:29.033544    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:29 minikube kubelet[1331]: E0803 09:11:29.134776    1331 kubelet.go:2419] "Error getting node" err="node \"minikube\" not found"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.235787    1331 kuberuntime_manager.go:1095] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.237349    1331 kubelet_network.go:60] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.263588    1331 kubelet_node_status.go:108] "Node was previously registered" node="minikube"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.263921    1331 kubelet_node_status.go:73] "Successfully registered node" node="minikube"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.518011    1331 apiserver.go:52] "Watching apiserver"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.524233    1331 topology_manager.go:200] "Topology Admit Handler"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.524500    1331 topology_manager.go:200] "Topology Admit Handler"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.524735    1331 topology_manager.go:200] "Topology Admit Handler"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.524917    1331 topology_manager.go:200] "Topology Admit Handler"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.525135    1331 topology_manager.go:200] "Topology Admit Handler"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.525342    1331 topology_manager.go:200] "Topology Admit Handler"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.525516    1331 topology_manager.go:200] "Topology Admit Handler"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.525691    1331 topology_manager.go:200] "Topology Admit Handler"
Aug 03 09:11:29 minikube kubelet[1331]: W0803 09:11:29.552774    1331 watcher.go:93] Error while processing event ("/sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod02ce4d38_a543_46ae_88c1_6ab0d547258f.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod02ce4d38_a543_46ae_88c1_6ab0d547258f.slice: no such file or directory
Aug 03 09:11:29 minikube kubelet[1331]: W0803 09:11:29.581381    1331 watcher.go:93] Error while processing event ("/sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod35e76fbb_e32f_4805_94e1_799d316f2c32.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/devices/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod35e76fbb_e32f_4805_94e1_799d316f2c32.slice: no such file or directory
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.628103    1331 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/deb97180-4ae7-4d63-86f2-d6b472a6fddf-lib-modules\") pod \"kube-proxy-czg8d\" (UID: \"deb97180-4ae7-4d63-86f2-d6b472a6fddf\") " pod="kube-system/kube-proxy-czg8d"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.628420    1331 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-7vf7p\" (UniqueName: \"kubernetes.io/projected/02ce4d38-a543-46ae-88c1-6ab0d547258f-kube-api-access-7vf7p\") pod \"mongo-deployment-7d4d5c9f6c-lvcvp\" (UID: \"02ce4d38-a543-46ae-88c1-6ab0d547258f\") " pod="default/mongo-deployment-7d4d5c9f6c-lvcvp"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.628747    1331 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/46850c4e-87a2-497b-b0bf-76e44ced9a6c-config-volume\") pod \"coredns-6d4b75cb6d-kptxw\" (UID: \"46850c4e-87a2-497b-b0bf-76e44ced9a6c\") " pod="kube-system/coredns-6d4b75cb6d-kptxw"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.628957    1331 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-lqfg6\" (UniqueName: \"kubernetes.io/projected/ba76e9c3-ce0c-434c-a39d-6ddfd0c15ac0-kube-api-access-lqfg6\") pod \"kubernetes-dashboard-5fd5574d9f-s2wmh\" (UID: \"ba76e9c3-ce0c-434c-a39d-6ddfd0c15ac0\") " pod="kubernetes-dashboard/kubernetes-dashboard-5fd5574d9f-s2wmh"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.629276    1331 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vwqfq\" (UniqueName: \"kubernetes.io/projected/15b175ce-330a-4295-9457-4b48c90905ea-kube-api-access-vwqfq\") pod \"redis-794bfd9659-mpdr2\" (UID: \"15b175ce-330a-4295-9457-4b48c90905ea\") " pod="default/redis-794bfd9659-mpdr2"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.629369    1331 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/deb97180-4ae7-4d63-86f2-d6b472a6fddf-kube-proxy\") pod \"kube-proxy-czg8d\" (UID: \"deb97180-4ae7-4d63-86f2-d6b472a6fddf\") " pod="kube-system/kube-proxy-czg8d"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.629434    1331 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-q68zl\" (UniqueName: \"kubernetes.io/projected/f3407c06-5663-4d84-9950-e5a63eb74594-kube-api-access-q68zl\") pod \"hello-minikube-66d5c9996d-jpbcf\" (UID: \"f3407c06-5663-4d84-9950-e5a63eb74594\") " pod="default/hello-minikube-66d5c9996d-jpbcf"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.629498    1331 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/ba76e9c3-ce0c-434c-a39d-6ddfd0c15ac0-tmp-volume\") pod \"kubernetes-dashboard-5fd5574d9f-s2wmh\" (UID: \"ba76e9c3-ce0c-434c-a39d-6ddfd0c15ac0\") " pod="kubernetes-dashboard/kubernetes-dashboard-5fd5574d9f-s2wmh"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.629528    1331 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dq6pt\" (UniqueName: \"kubernetes.io/projected/7af5d6a3-ce37-4104-aa97-2cff57bd9658-kube-api-access-dq6pt\") pod \"dashboard-metrics-scraper-78dbd9dbf5-bg74g\" (UID: \"7af5d6a3-ce37-4104-aa97-2cff57bd9658\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-78dbd9dbf5-bg74g"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.629554    1331 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp-volume\" (UniqueName: \"kubernetes.io/empty-dir/7af5d6a3-ce37-4104-aa97-2cff57bd9658-tmp-volume\") pod \"dashboard-metrics-scraper-78dbd9dbf5-bg74g\" (UID: \"7af5d6a3-ce37-4104-aa97-2cff57bd9658\") " pod="kubernetes-dashboard/dashboard-metrics-scraper-78dbd9dbf5-bg74g"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.629579    1331 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/35e76fbb-e32f-4805-94e1-799d316f2c32-tmp\") pod \"storage-provisioner\" (UID: \"35e76fbb-e32f-4805-94e1-799d316f2c32\") " pod="kube-system/storage-provisioner"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.629610    1331 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/deb97180-4ae7-4d63-86f2-d6b472a6fddf-xtables-lock\") pod \"kube-proxy-czg8d\" (UID: \"deb97180-4ae7-4d63-86f2-d6b472a6fddf\") " pod="kube-system/kube-proxy-czg8d"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.629665    1331 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-7khwq\" (UniqueName: \"kubernetes.io/projected/deb97180-4ae7-4d63-86f2-d6b472a6fddf-kube-api-access-7khwq\") pod \"kube-proxy-czg8d\" (UID: \"deb97180-4ae7-4d63-86f2-d6b472a6fddf\") " pod="kube-system/kube-proxy-czg8d"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.629755    1331 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zxx7c\" (UniqueName: \"kubernetes.io/projected/35e76fbb-e32f-4805-94e1-799d316f2c32-kube-api-access-zxx7c\") pod \"storage-provisioner\" (UID: \"35e76fbb-e32f-4805-94e1-799d316f2c32\") " pod="kube-system/storage-provisioner"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.629782    1331 reconciler.go:270] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-v5gzm\" (UniqueName: \"kubernetes.io/projected/46850c4e-87a2-497b-b0bf-76e44ced9a6c-kube-api-access-v5gzm\") pod \"coredns-6d4b75cb6d-kptxw\" (UID: \"46850c4e-87a2-497b-b0bf-76e44ced9a6c\") " pod="kube-system/coredns-6d4b75cb6d-kptxw"
Aug 03 09:11:29 minikube kubelet[1331]: I0803 09:11:29.629796    1331 reconciler.go:157] "Reconciler: start to sync state"
Aug 03 09:11:30 minikube kubelet[1331]: I0803 09:11:30.955837    1331 request.go:601] Waited for 1.199724213s due to client-side throttling, not priority and fairness, request: POST:https://control-plane.minikube.internal:8443/api/v1/namespaces/kubernetes-dashboard/serviceaccounts/kubernetes-dashboard/token
Aug 03 09:11:35 minikube kubelet[1331]: I0803 09:11:35.499505    1331 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="025b31a093d97afdbb7c2691873fa124f3b76092ad0a12e4435bdcd467ba5d5f"
Aug 03 09:11:37 minikube kubelet[1331]: I0803 09:11:37.121191    1331 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="9363542f4ace5115c7dcd5ec0b64369ccc257e23c4514a541932d710633b7fc0"
Aug 03 09:11:37 minikube kubelet[1331]: I0803 09:11:37.315939    1331 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="cabaf417a3fedc093ce3a930e7ceffc72b5038138cb0e84bb3c43c2f7f64aa91"
Aug 03 09:11:38 minikube kubelet[1331]: I0803 09:11:38.003226    1331 pod_container_deletor.go:79] "Container not found in pod's containers" containerID="7e80885b0e877b878a1dc45b549fae3082cedb382360a2837703520c06b05a1c"
Aug 03 09:12:06 minikube kubelet[1331]: I0803 09:12:06.621867    1331 scope.go:110] "RemoveContainer" containerID="19f9083e6d2f4be84c1ea53957f4b1b5697a3acc86686de643d45cba2dd17d44"
Aug 03 09:12:06 minikube kubelet[1331]: I0803 09:12:06.622688    1331 scope.go:110] "RemoveContainer" containerID="7c94a77ad61da4b64edc78e95d6ac06256ffca1ff2599756962e60aff62a8449"
Aug 03 09:12:06 minikube kubelet[1331]: E0803 09:12:06.622854    1331 pod_workers.go:951] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(35e76fbb-e32f-4805-94e1-799d316f2c32)\"" pod="kube-system/storage-provisioner" podUID=35e76fbb-e32f-4805-94e1-799d316f2c32
Aug 03 09:12:18 minikube kubelet[1331]: I0803 09:12:18.684547    1331 scope.go:110] "RemoveContainer" containerID="7c94a77ad61da4b64edc78e95d6ac06256ffca1ff2599756962e60aff62a8449"

* 
* ==> kubernetes-dashboard [10cd3c49d421] <==
* 2022/07/31 11:59:13 Using namespace: kubernetes-dashboard
2022/07/31 11:59:13 Using in-cluster config to connect to apiserver
2022/07/31 11:59:13 Using secret token for csrf signing
2022/07/31 11:59:13 Initializing csrf token from kubernetes-dashboard-csrf secret
2022/07/31 11:59:13 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2022/07/31 11:59:13 Successful initial request to the apiserver, version: v1.24.1
2022/07/31 11:59:13 Generating JWE encryption key
2022/07/31 11:59:13 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2022/07/31 11:59:13 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2022/07/31 11:59:14 Initializing JWE encryption key from synchronized object
2022/07/31 11:59:14 Creating in-cluster Sidecar client
2022/07/31 11:59:14 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2022/07/31 11:59:14 Serving insecurely on HTTP port: 9090
2022/07/31 11:59:44 Successful request to sidecar
2022/07/31 11:59:13 Starting overwatch

* 
* ==> kubernetes-dashboard [cb2136acb120] <==
* 2022/08/03 09:11:37 Starting overwatch
2022/08/03 09:11:37 Using namespace: kubernetes-dashboard
2022/08/03 09:11:37 Using in-cluster config to connect to apiserver
2022/08/03 09:11:37 Using secret token for csrf signing
2022/08/03 09:11:37 Initializing csrf token from kubernetes-dashboard-csrf secret
2022/08/03 09:11:37 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2022/08/03 09:11:37 Successful initial request to the apiserver, version: v1.24.1
2022/08/03 09:11:37 Generating JWE encryption key
2022/08/03 09:11:37 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2022/08/03 09:11:37 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2022/08/03 09:11:38 Initializing JWE encryption key from synchronized object
2022/08/03 09:11:38 Creating in-cluster Sidecar client
2022/08/03 09:11:38 Metric client health check failed: the server is currently unable to handle the request (get services dashboard-metrics-scraper). Retrying in 30 seconds.
2022/08/03 09:11:38 Serving insecurely on HTTP port: 9090
2022/08/03 09:12:08 Successful request to sidecar

* 
* ==> storage-provisioner [4ad9b04f7a09] <==
* I0803 09:12:18.950229       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0803 09:12:18.975852       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0803 09:12:18.976316       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0803 09:12:36.442038       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0803 09:12:36.442717       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_900d2d8b-8b13-4445-ba94-11c4ed4ccf27!
I0803 09:12:36.443453       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"9f614615-0ff4-4611-a035-38bf77c18b95", APIVersion:"v1", ResourceVersion:"21151", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_900d2d8b-8b13-4445-ba94-11c4ed4ccf27 became leader
I0803 09:12:36.544979       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_900d2d8b-8b13-4445-ba94-11c4ed4ccf27!

* 
* ==> storage-provisioner [7c94a77ad61d] <==
* I0803 09:11:36.191427       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0803 09:12:06.227171       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

